# Algoritmi di ordinamento

Gli algoritmi di ordinamento possono essere classificati in base alla loro complessità temporale e al tipo di operazioni che utilizzano. Tipicamente, gli algoritmi di ordinamento hanno una complessità di $O(n^2)$ o $O(n \log n)$ quando sono basati sul confronto. Esistono anche algoritmi di ordinamento che non si basano sul confronto e che possono raggiungere una complessità di $O(n)$.

## MergeSort

*MergeSort* è un algoritmo di ordinamento che utilizza la strategia *divide et impera*. L'idea è scomporre il problema iniziale in sottoproblemi più piccoli, risolvere tali sottoproblemi ricorsivamente e poi unire le soluzioni. In pratica, l'algoritmo dimezza l'*array*, applica l'algoritmo ai sottoarray e poi fonde le sottosequenze ordinate. L'algoritmo continua a dividere finché non rimangono solo *array* di 1 o 2 elementi. Ad esempio, partendo dall'array `[52804719326]`, la divisione procede come segue:

```
[528047] [19326]
[528] [047]
[52] [8]
[5] [2]
```

Una volta che l'*array* è stato completamente suddiviso, inizia la fase di fusione (*merge*). Ad esempio, partendo dai sottoarray `5 2`, la fusione procede come segue:

```
[25] [8]
[258]
```

La fusione avviene scorrendo le due liste con due indici, confrontando le coppie di elementi nelle due liste e scegliendo quale elemento mettere per primo nell'*array* fuso. Questa procedura funziona correttamente solo se gli *array* di partenza sono già ordinati.

Ecco lo pseudocodice per l'algoritmo *MergeSort* operando su un array di dimensione $n$:

```
MergeSort(A, p, r)
    if p < r then
        q <- floor((p + r) / 2)
        MergeSort(A, p, q)    // [n/2] elementi
        MergeSort(A, q + 1, r)  // [n/2] elementi
        Merge(A, p, q, r)
```

La procedura `Merge` funziona come segue:

1. Estrai ripetutamente il minimo tra gli elementi dei sottoarray `A[p..q]` e `A[q+1..r]` e copialo in un array di output `C` fino a quando uno dei due sottoarray non si svuota.
2. Copia gli elementi rimasti dal sottoarray non svuotato in `C`.

L'algoritmo MergeSort non opera in loco, poiché richiede un array di output `C`. La complessità temporale della procedura `Merge` è lineare, ovvero $\Theta(n)$, dove $n = r - p + 1$.

```
merge(A,p,q,r)
	n_1 <- q - p + 1
	n_2 <- r - q
	for i <- 1 to n_1 do
		L[i] <- A[p+i-1]
	for j <- 1 to n_2 do
		R[j] <- A[q+j]
	L[n_1 + 1] <- R[n_2 + 1] <- infinity
	i <- j <- 1
	for k <- p to r do
		if L[i] <= R[j] then
			A[k] <- L[i]
			i <- i + 1
		else
			A[k] <- R[j
```

I costi del *merge* possono essere suddivisi in tre componenti principali. L'assegnamento di $q$ è lineare, mentre la divisione risulta poco costosa. Il passo più oneroso è rappresentato dalla fusione.

Il numero di confronti può essere espresso dalla seguente equazione:
$$T(n) = d(n) + 2 \cdot T\left(\frac{n}{2}\right) + c(n) $$
dove $d$ rappresenta il costo di divisione, che è $\Theta(1)$, e $c(n)$ rappresenta il costo di fusione, che è $\Theta(n)$. Di conseguenza, il costo totale rientra nella classe $\Theta(n)$.

Applicando il teorema Master con $a = 2$ e $b = 2$, si ottiene
$$T(n) = \Theta(n \log n). $$
Pertanto, il *MergeSort* risulta ottimale dal punto di vista temporale, ma non lo è dal punto di vista della memoria, poiché non può essere eseguito in loco.

## QuickSort

*QuickSort* è un algoritmo di ordinamento che divide il problema in due sottoproblemi scegliendo un elemento *pivot* e separando gli elementi maggiori e minori rispetto a questo pivot.

La versione non *in loco* di QuickSort funziona come segue:

1. Scegli un elemento $x$ (il *pivot*) dall'array $A$.
2. Partiziona $A$ rispetto a $x$ calcolando due sotto-array:
   - $A_1$ contiene tutti gli elementi di $A$ che sono minori o uguali a $x$.
   - $A_2$ contiene tutti gli elementi di $A$ che sono maggiori di $x$.
3. Se $A_1$ contiene più di un elemento, applica QuickSort a $A_1$.
4. Se $A_2$ contiene più di un elemento, applica QuickSort a $A_2$.
5. Copia la concatenazione di $A_1$ e $A_2$ in $A$.

La partizione *in loco* di QuickSort avviene scorrendo l'array da sinistra verso destra e da destra verso sinistra. Durante questa scansione, ci si ferma su un elemento maggiore del pivot quando si scorre da sinistra verso destra, e su un elemento minore del pivot quando si scorre da destra verso sinistra. A questo punto, si scambiano i due elementi e si continua finché gli indici non si incrociano. Una volta che gli indici si incrociano, si pongono i due sotto-array a sinistra e a destra del pivot.

Per implementare QuickSort in loco, è necessario definire una funzione ausiliaria chiamata `Partition(A, i, f)`:

1. $x = A[i]$ (partiziona $A[i..f]$ intorno al pivot $A[i]$).
2. Inizializza $\inf = i$ e $\sup = f + 1$.
3. Esegui un ciclo infinito:
   - Incrementa $\inf$ finché $\inf \leq f$ e $A[\inf] \leq x$.
   - Decrementa $\sup$ finché $A[\sup] > x$.
   - Se $\inf < \sup$, scambia $A[\inf]$ e $A[\sup]$.
   - Altrimenti, esci dal ciclo.
4. Scambia $A[i]$ e $A[\sup]$.
5. Restituisci $\sup$.

Alla riga 5, la condizione è sufficiente perché l'indice non uscirà mai dall'array senza prima incontrare il pivot e bloccarsi. La riga 8 posiziona il pivot al centro. La riga 9 restituisce il nuovo pivot. Il tempo di esecuzione di QuickSort è $\Theta(n)$ ad ogni iterazione, poiché vengono effettuati $n-1$ confronti tra gli elementi e il pivot.

Ora possiamo scrivere l'implementazione completa di QuickSort:

```
QuickSort(A, i, f)
    if (i >= f) then return
    m = Partition(A, i, f)
    QuickSort(A, i, m-1)
    QuickSort(A, m+1, f)
```

Questa versione di QuickSort non è stabile, poiché l'albero delle chiamate ricorsive può essere sbilanciato a seconda della distribuzione dell'array di origine rispetto al pivot. Una possibile ottimizzazione è la scelta dell'elemento mediano come pivot iniziale.

\underline{Caso peggiore}

Nel caso peggiore, il pivot è il minimo o il massimo dell'*array* (*array* ordinato direttamente o inversamente). Il numero di confronti è:

$$ T(n) = \Theta(n) + $$

Utilizzando il metodo dell'*albero della ricorsione*, sommiamo i nodi di tutti i livelli e notiamo che sono:


$$T(n) = \sum_{i=2}^n i + \cancel{n} = \Theta(n^2)$$


\underline{Caso migliore}

Nel caso migliore, l'albero di ricorsione è perfettamente bilanciato (due sottoalberi di dimensione non maggiore di $n/2$), e l'algoritmo ha un costo coincidente a quello del *MergeSort* (pseudolineare in $n$):

$$ T(n) = \Theta(n \log n) $$

\underline{Caso medio}

Per ottenere un'approssimazione del caso medio di *QuickSort*, possiamo scegliere un *pivot* dall'*array* in modo casuale. Questo riduce la possibilità di un comportamento peggiore del previsto. Possiamo ulteriormente migliorare la scelta del *pivot* selezionando il mediano di un sottoinsieme di elementi estratti casualmente.

Nel caso di una sola estrazione, ogni elemento ha una probabilità di $\frac{1}{n}$ di essere scelto come pivot. Il numero $C$ di confronti nel caso atteso è dato da:

$$ C(n) = \sum_{a=0}^{n-1} \frac{1}{n} \left[ n-1 + C(a) + C(n-a-1) \right] = n-1 + \sum_{a=0}^{n-1} \frac{2}{n} C(a) $$

dove $a$ e $(n-a-1)$ sono le dimensioni dei sottoproblemi risolti ricorsivamente. $C(a)$ e $C(n-a-1)$ danno luogo alla stessa sommatoria perché entrambe le chiamate alternano partizioni buone e cattive. La relazione di ricorrenza $C(n) = n-1 + \sum_{a=0}^{n-1} \frac{2}{n} C(a)$ ha soluzione $C(n) \leq 2n \log n$, quindi:

$$ T(n) = O(n \log n) $$

Non possiamo usare $\Theta$ perché la soluzione è ottenuta per integrazione e non è dunque possibile ottenere un limite stretto.

Java implementa *QuickSort*, *MergeSort* e *HeapSort* come funzioni nelle classi `Arrays` e `Collections`. Queste implementazioni sono ottimizzate per garantire prestazioni efficienti e affidabili in una vasta gamma di scenari.


## Algoritmi lineari

Gli algoritmi di ordinamento che funzionano in tempo lineare in $n$ hanno condizioni particolari di applicabilità. Tra questi, troviamo l'IntegerSort (o CountingSort), il BucketSort e il RadixSort.

### IntegerSort

L'IntegerSort funziona esclusivamente su vettori $X$ di numeri interi di cui siano noti il minimo e il massimo. I valori devono essere compresi nell'intervallo $[1, k]$. L'algoritmo costruisce un array di appoggio $Y$ di $k$ elementi, inizializzato a zero. Scorrendo $X$, per ogni occorrenza del numero $n$, incrementa di uno l'$n$-esima cella di $Y$. Al termine dello scorrimento, ricostruisce $X$ copiando al suo interno gli indici di $Y$, ognuno un numero di volte pari al numero contenuto nella rispettiva cella.

Osserviamo lo pseudocodice:
```
IntegerSort(X, k)
1. sia Y un array di dimensione k
2. for i = 1 to k
3.    Y[i] = 0
4. for j = 1 to n
5.    Y[X[j]] = Y[X[j]] + 1
6. k = 1
7. for i = 1 to k
8.    while Y[i] > 0
9.       X[k] = i
10.      Y[i] = Y[i] - 1
11.      k = k + 1
```

Analizziamo la complessità temporale dell'algoritmo:
- Il tempo per inizializzare $Y$ a zero è $O(k)$.
- Il tempo per contare gli indici è $O(n)$.
- Il ciclo esterno della ricostruzione è compatto e viene eseguito $k$ volte.
- I cicli interni sono indefiniti, ma le iterazioni totali saranno $n$.

In sintesi, l'algoritmo ha una complessità temporale di $O(k + n)$, che si approssima a $O(n)$ se $k \leq n$. Non essendo basato sul confronto, non ha il limite inferiore $\Omega(n \log n)$.

### Stabilità degli algoritmi

Un algoritmo è definito *stabile* se preserva l'ordine iniziale tra elementi con la stessa chiave. La stabilità è particolarmente utile per ordinamenti su più chiavi. Applicando algoritmi stabili, è possibile ordinare prima per una chiave e poi per un'altra; se l'algoritmo non è stabile, gli elementi rimarranno ordinati solo per l'ultima chiave applicata.
