## Alberi di ricerca

Un *albero binario di ricerca* (*binary search tree*, BST) è un albero binario che soddisfa le seguenti proprietà:

 1. ogni nodo $v$ contiene un elemento $elem(v)$ cui è associata una $chiave(v)$ presa da un dominio totalmente ordinato
 2. le chiavi nel sottoalbero sinistro di $v$ sono minori o uguali alla $chiave(v)$
 3. le chiavi nel sottoalbero destro di v sono maggiori o uguali alla $chiave(v)$

Gli alberi binari di ricerca sono utilizzati per implementare con efficienza i *dizionari*, definiti come collezioni di coppie chiave/valore. In questo caso, si chiede che le chiavi da ordinare tramite l'albero siano univoche. In casi d'uso che non richiedano chiavi univoche, possono esserci varie altre regole per ordinare gli elementi.


Su un albero binario generico il costo della ricerca è $O(n)$ indipendentemente dalla strategia applicata, che sia in profondità o in ampiezza. Se l'albero è di ricerca, possiamo partire dalla radice e decidere in che verso muoverci sfruttando l'ordinamento delle chiavi. L'algoritmo di ricerca, formalmente, è:

```
algoritmo search(chiave k) -> elem
	v <- radice di T
	while (v != null) do
		if (k = chiave(v)) then return elem(v)
		else if (k < chiave(v)) then v <- figlio sinistro di v
		else v <- figlio destro di v
	return null
```

Il caso peggiore si verifica quando la chiave desiderata corrisponde ad una foglia a massima profondità, o quando essa è introvabile. In tal caso, il numero di passi è pari all'altezza dell'albero, e la complessità è $O(h)$, dove $h$ è l'altezza dell'albero. Possiamo mettere in relazione l'altezza $h$ dell'albero con il suo numero $n$ di elementi, per ottenere una stima di complessità in funzione di quest'ultimo. Il caso peggiore corrisponde alla ricerca sull'albero degenere linearizzato, nel quale ogni elemento ha un solo figlio. In questo caso l'albero degenera in una lista collegata con $h = n-1$ elementi, e la ricerca ha dunque complessità $O(n)$ perché richiede la scansione lineare. L'altezza è $h = \Theta(n)$. Il caso migliore è un albero completo bilanciato, in cui ogni elemento ha entrambi i figli, tranne le foglie, e in cui i sottoalberi destro e sinistro di ogni elemento hanno lo stesso numero di membri. In tal caso, l'altezza dell'albero è $h = \Theta(\log(n))$. 

Valendo l'ordinamento gerarchico delle chiavi, gli elementi di chiave minima e massima saranno rispettivamente il più a sinistra e il più a destra dell'albero. L'algoritmo per trovare il minimo a partire dalla radice deve procedere verso sinistra fino a una foglia. L'algoritmo per trovare il massimo a partire dalla radice deve procedere verso destra fino a una foglia.

```
algoritmo max(nodo u) -> nodo
	v <- u
	while(figlio destro di v != null) do
		v <- figlio destro di v
	return v

algoritmo min(nodo u) -> nodo
	v <- u
	while(figlio sinistro di v != null) do
		v <- figlio sinistro di v
	return v
```


Chiamiamo *predecessore* di un nodo $u$ in un BST il nodo $v$ nell'albero di chiave massima minore o uguale a $chiave(u)$. Chiamiamo *successore* di un nodo $u$ in un BST il nodo $v$ nell'albero di chiave minima maggiore o uguale a $chiave(u)$. Algoritmicamente, il successore è il minimo del sottoalbero destro, se esiste. Se il sottoalbero destro non esiste, si risale l'albero fino ad un antenato che sia figlio sinistro del proprio nodo padre. Tale nodo padre sarà l'elemento sul cui sottoalbero destro sarà possibile trovare il successore dell'elemento desiderato. Il predecessore è il massimo del sottoalbero sinistro, se esiste. Se il sottoalbero sinistro non esiste, si risale lungo l'albero fino a trovare un antenato che sia figlio destro del proprio nodo padre, e si cerca il predecessore dell'elemento desiderato nel sottoalbero sinistro dell'antenato. Questo algoritmo richiede ovviamente di salvare un riferimento al padre in ogni nodo dell'albero.

```
algoritmo pred(nodo u) -> nodo
	if (u ha un figlio sinistro sin(u)) then
		return max(sin(u))
	while (parent(u) != null e u è figlio sinistro di suo padre) do
		u <- parent(u)
	return parent(u)
```


L'inserimento in un albero binario di ricerca va effettuato mantenendo la proprietà di ricerca. L'operazione di inserimento è inizialmente simile a quella di ricerca, perché risulta necessario trovare il punto dove inserire ordinatamente il nuovo elemento. Questi è sempre aggiunto come foglia, nel modo meno invasivo possibile. Anche l'inserimento, come la ricerca, ha complessità $O(h)$. La visita in profondità in ordine simmetrico riflette l'ordinamento delle chiavi.

Cancellare mantenendo la proprietà di ricerca è più complesso. Se l'elemento da eliminare è una foglia, può essere rimosso direttamente senza bisogno di altre considerazioni. Se l'elemento da eliminare ha un solo figlio, attacchiamo il figlio dell'elemento da eliminare al padre dell'elemento da eliminare, al posto dell'eliminato. Se l'elemento da eliminare ha due figli, deve essere sostituito con il suo predecessore, e quest'ultimo va poi eliminato secondo le regole dei casi precedenti. Questa procedura funziona perché, per definizione, il predecessore non ha figli destri. In questo modo, la sua eliminazione ricade necessariamente in uno dei primi due casi. La procedura può essere effettuata anche utilizzando l'elemento successore al posto del predecessore.

L'efficienza $O(h)$, come spiegato in precedenza si traduce, in termini di $n$, in $O(n)$ nel caso peggiore per alberi molto sbilanciati, e in $O(\log (n))$ nel caso peggiore per alberi bilanciati. A tal fine, definiamo il *fattore di bilanciamento* $\beta(v) = |h(\mathrm{sinistro}(v)) - h(\mathrm{destro}(v))|$. Definiamo un albero *bilanciato in altezza* se $\forall v$ vale $\beta(v) \leq 1$, *completo* se $\forall v$ $\beta(v) = 0$ e degenerato in lista se $\beta(v) = h$. Esiste una categoria di alberi, chiamata AVL, che comprende alberi binari bilanciati in altezza, che si auto-aggiustano per rotazione. La rotazione consiste nello spostare la posizione gerarchica di nodi senza però alterare gli archi. Ad esempio, un nodo figlio con un solo figlio può essere promosso a nodo con due figli (il figlio originale e il padre) alzandolo di un livello, e abbassando il padre. La rotazione ha costo $O(1)$. L'inserimento con rotazione richiede di calcolare i fattori di bilanciamento lungo il percorso tra la radice e il nuovo nodo inserito. Successivamente, in caso di criticità solitamente riconoscibili da uno sbilanciamento sopra la soglia di 2, si effettua una singola rotazione nel verso opportuno. Strategie di bilanciamento alternative alla rotazione comprendono lo spostamento e la fusione. Gli alberi rosso-neri utilizzano entrambe queste strategie.
