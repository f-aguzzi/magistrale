
## B-Alberi

I B-alberi sono alberi bilanciati utilizzati per memorizzare grandi quantità di dati su disco. Sono particolarmente efficaci per le basi di dati, poiché sono progettati per minimizzare il numero di accessi necessari. Le operazioni fondamentali che possono essere eseguite su un B-albero includono l'inserimento, la cancellazione e la ricerca di dati. Inoltre, per mantenere l'albero bilanciato, vengono eseguite operazioni di bilanciamento come la divisione e l'unione dei nodi.

Ogni nodo di un B-albero contiene più chiavi. I nodi possono contenere fino a $n$ chiavi e avere $n+1$ figli. Una lettura su disco $DiskRead(x)$ o una scrittura $DiskWrite(x)$ richiede un tempo di accesso proporzionale alla quantità di dati, nell'ordine dei millisecondi, il che è molto più lento rispetto a un'operazione su CPU. Utilizzare un albero molto ramificato permette di ridurre l'altezza dell'albero e, di conseguenza, il numero di accessi alla memoria. Ad esempio, se $n+1 = 1000$, è possibile contenere $10^9-1$ chiavi in un albero di altezza 2.

Un B-albero $T$ è un albero con radice $root[T]$ che soddisfa le seguenti proprietà:

1. Ogni nodo $x$ contiene vari campi, tra cui:
   - $n[x]$: il numero di chiavi presenti nel nodo.
   - $n[x]+1$: il numero di figli del nodo.
   - Altri campi specifici che possono essere recuperati dalle slide.

2. Se il nodo non è una foglia, contiene anche i puntatori ai suoi figli.

3. Le $n(x)$ chiavi di un nodo interno separano gli intervalli contenenti le chiavi dei sottoalberi, rispettando la proprietà degli intervalli.

4. Tutte le foglie si trovano allo stesso livello $h$, corrispondente all'altezza dell'albero.

5. Il numero di chiavi in un nodo è limitato da una costante $t$, detta grado minimo dell'albero:
   - Ogni nodo, eccetto la radice, ha almeno $t-1$ chiavi e $t$ figli: $$n[x] \geq t-1$$.
   - Se l'albero non è vuoto, la radice contiene almeno una chiave; se la radice non è una foglia, ha almeno due figli.
   - Un nodo può contenere al massimo $2t-1$ chiavi, nel qual caso è considerato pieno e deve essere suddiviso.

Indicando con $k_j$ una qualsiasi chiave memorizzata nel sottoalbero $C_j[x]$, la proprietà dei sottointervalli ci dice che, per $j = 1, \ldots, n[x] + 1$:

$$k_j \leq key_i[x] \leq k_{i+1}, \quad i = 1, \ldots, n[x]$$

I B-alberi minimi, con $t = 2$, sono noti come alberi 2-3-4.

Ogni B-albero con grado minimo $t$ che contiene $N$ chiavi avrà un'altezza $h$ che sarà al massimo $\log_t \frac{N+1}{2}$, assumendo per convenzione $h = -1$ per gli alberi vuoti.

\underline{Dimostrazione}:

- Passiamo alla versione esponenziale della disuguaglianza: $h \leq \log_t \frac{N+1}{2} \to N \geq 2t^h - 1$.
- Se l'albero è vuoto, $h = -1$ e $N = 0 \geq 2t^{-1} - 1$ (caso base).
- Supponiamo che l'albero non sia vuoto, con radice $r$ e $h \geq 0$. Sia $m_i$ il numero di nodi al livello $i$-esimo. Allora:
  - $m_0 = 1$
  - $m_1 = n[root] + 1 \geq 2$ (figli della radice che ha almeno una chiave)
  - $m_i \geq t m_{i-1}$ per $i > 1$, quindi $m_i \geq t^{i-1} m_1 \geq 2t^{i-1}$ (ogni nodo ha almeno $t$ figli)

Quindi, per le chiavi:

$$ N = \sum_x n[x] $$
$$ = n[root] + \sum_{i=1}^h \sum_{x~di~livello~i} n[x] $$
$$ \geq 1 + \sum_{i=1}^h 2t^{i-1}(t-1) $$
(che è una serie geometrica)
$$ = 1 + 2(t-1) \frac{t^h-1}{t-1} $$
$$ = 1 + 2(t^h-1) = 2t^h - 1. $$

L'altezza di un B-albero è $O(\log_t N)$, dello stesso ordine di grandezza di $O(\log_2 N)$ degli alberi binari, ma per $50 \leq t \leq 2000$ la notazione nasconde un fattore di riduzione compreso tra 5 e 11.

### Operazioni Elementari e Convenzioni

Le operazioni elementari su un B-albero seguono alcune convenzioni fondamentali:

- la radice dell'albero è sempre mantenuta in memoria;
- i nodi passati come parametri alle funzioni sono sempre letti dalla memoria.

\underline{Operazioni Definite}

Le operazioni principali definite per un B-albero includono:

- costruttore di un albero vuoto (`BTree`): inizializza un B-albero vuoto
- `search`: cerca una chiave specifica all'interno dell'albero
- `insert`: inserisce una nuova chiave nell'albero
- `delete`: rimuove una chiave esistente dall'albero

\underline{Procedure Ausiliarie}

Per supportare le operazioni principali, vengono utilizzate tre procedure ausiliarie:

- `SearchSubtree`: Cerca una chiave all'interno di un sottoalbero specifico.
- `SplitChild`: Divide un nodo figlio pieno in due nodi, ridistribuendo le chiavi e aggiornando i puntatori ai figli.
- `InsertNonfull`: Inserisce una chiave in un nodo che non è pieno, mantenendo l'albero bilanciato.

Queste procedure ausiliarie sono essenziali per garantire che l'albero rimanga bilanciato e che le operazioni di inserimento e cancellazione siano efficienti.

\underline{Implementazioni e complessità}

Il costruttore `BTree`:
```
BTree(t)
	root[T] <- nil
```
ha complessità $O(1)$.

L'operazione di ricerca `Search`:
```
Search(T,k)
	if root[T] = nul then return nil
	else return SearchSubtree(root[T], k)
```
si basa sull'operazione ausiliaria `SearchSubTree`:
```
SearchSubtree(x, k)
	i <- 1
	while i<= n[x] and k>key_i[x] do
		i <- i+1 // ricerca dell'indice i tale che k <= key_i [x]
		         // invariante: proprietà degli intervalli
		if i <= n[x] and k = key_i [x] return x, i // successo
	else if leaf[x] return nil // ricerca senza successo
	else // ricerca ricorsiva nel sottoalbero c_i[x]
		DiskRead(c_i[x])
		return SearchSubtree(c_i[x], k)
```
che prende un nodo come argomento. Il costo complessivo va calcolato sommando il costo ricorsivo della discesa al costo additivo della ricerca sequenziale o binaria sulle chiavi del nodo considerato. La versione binaria prende la seguente forma:
```
SearchSubtree(x, k)
	i <- 1, j <- n[x]+1
	while i < j do
		if k <= key_floor((i+j)/2) [x] then j <- floor((i+j)/2)
		else i <- floor((i+j)/2) + 1
	if leaf[x] return nil // ricerca senza successo
	else // ricerca ricorsiva nel sottoalbero c_i[x]
		DiskRead(c_i[x])
		return SearchSubtree(c_i[x], k)
```

Il numero di $DiskRead$ è, al più, uguale all'altezza $h$ dell'albero, ed è quindi $O(h) = O(\log_t N)$ con $N$ chiavi nel B-albero.
Il tempo $T$ di CPU per la ricerca è $T=O(th) = O(t \log_t N)$. Usando la ricerca dicotomica, il costo è $O(\log t h)$ e complessivamente $O(\log t \log_t N) = O(\log N)$.


L'inserimento di una chiave in un B-albero segue un processo specifico. Non si aggiunge mai ai nodi interni. Le chiavi vengono inserite solo nelle foglie. Questo perché l'inserimento nei nodi interni creerebbe sottoalberi, complicando la gestione dell'albero. Una chiave viene inserita solo in una foglia. Se la foglia non è piena, l'inserimento avviene direttamente. Se la foglia è piena, il nodo viene diviso. Se la foglia è piena, viene divisa in due nodi più piccoli. L'elemento centrale viene spostato nel nodo padre, mantenendo l'ordinamento delle chiavi. La complessità dell'inserimento è $O(h)$, dove $h$ è l'altezza dell'albero. Questo è dovuto al fatto che ogni operazione di inserimento richiede un numero costante di accessi al disco (DiskRead e DiskWrite) tra due chiamate consecutive di `insertNonFull`. Consideriamo, ad esempio, un B-albero con grado minimo $t = 4$. In questo caso, il numero di chiavi in un nodo è compreso tra 3 e 7 (limite inferiore $t-1$, limite superiore $2t-1$). Se la radice è piena, si crea una nuova radice e si sposta la vecchia radice come sua figlia. Il nodo figlio viene quindi diviso in due nodi più piccoli, spostando l'elemento centrale nella nuova radice. Questo processo mantiene l'ordinamento delle chiavi, poiché le chiavi sono già ordinate.


La cancellazione di una chiave può lasciare un nodo con un numero di chiavi inferiore al minimo. In questo caso, si uniscono due nodi fratelli, inserendo come intermezzo la chiave intermedia presa dal padre. Questa operazione è speculare alla divisione (*split*). Le operazioni di inserimento e cancellazione in un B-albero sono progettate per mantenere l'albero bilanciato e ordinato, garantendo così un accesso efficiente ai dati. La complessità di queste operazioni è $O(h)$, dove $h$ è l'altezza dell'albero, grazie a un numero costante di accessi al disco tra le chiamate consecutive delle procedure ausiliarie.

## Tabelle hash

Un'altra struttura dati concreta per il tipo astratto `Dizionario` è la *tabella hash* o *tavola ad accesso diretto*. L'idea alla base delle tabelle *hash* è mappare direttamente la chiave all'indice di un *array*. Questo è semplice se le chiavi sono univoche e numeriche, purché non superino le dimensioni dell'*array*. I costi associati all'accesso a diverse strutture dati variano: una lista e un albero di ricerca non bilanciato hanno un costo di $O(n)$, mentre un albero di ricerca bilanciato ha un costo di $O(\log n)$. Le tabelle hash, invece, offrono un costo di $O(1)$.

Di seguito è riportato lo pseudocodice dell'implementazione di una tabella *hash*.
```
classe TavolaAccessoDiretto implementa Dizionario:
dati:
	un array v di dimensione m>=n in cui v[k] = elem se c'è un elemento con 
	chiave n nel dizionario, e v[k] = null altrimenti.
	Le chiavi k devono essere nell'intervallo [0, m-1].

operazioni:
	insert(elem e, chiave k)
		v[k] <- e
	delete(chiave k)
		v[k] <- null
	search(chiave k) -> elem
		return v[k]
```

Tutte e tre le operazioni hanno complessità temporale lineare: $T(n) = O(1)$. Il fattore di carico $\alpha$ è definito come il rapporto tra il numero di elementi $n$ e la capacità $m$ della struttura dati:
$$ \alpha = \frac n m $$

La *funzione hash* $h$ mappa un dominio totalmente ordinato $U$ (che può includere chiavi non numeriche) a un intervallo $[0, m-1]$. L'elemento con chiave $k$ viene posizionato in $v[h(k)]$. Un problema comune è la *gestione delle collisioni*: è difficile trovare una funzione *hash* $h$ che sia veramente univoca (*hash perfetta*), dove $u \neq v$ implica $h(u) \neq h(v)$. Questo è possibile solo se il numero di elementi in $U$ è minore o uguale a $m$, permettendo a $h$ di essere iniettiva. Solitamente, la funzione *hash* è suriettiva. Una collisione si verifica quando si inserisce un elemento la cui chiave ha un valore *hash* uguale a quello di un elemento già memorizzato. Per ridurre le collisioni, è possibile limitare l'insieme delle chiavi.

Per ottenere un dizionario con un tempo di accesso veramente $O(1)$, è necessario disporre di una funzione *hash* che sia *perfetta*, ovvero che mappi ogni chiave a un indice univoco senza collisioni, e calcolabile in tempo $O(1)$. Se queste condizioni non sono soddisfatte, è importante che la funzione *hash* garantisca almeno l'*uniformità semplice*, ovvero che ogni elemento abbia la stessa probabilità di causare una collisione.

Le funzioni *hash* crittografiche sono progettate per essere *unidirezionali*, il che significa che è difficile risalire al messaggio originale a partire dal valore *hash*. Queste funzioni sono resistenti alle collisioni, alla *preimmagine* (è difficile trovare un messaggio che corrisponda a un dato valore hash) e alla *seconda preimmagine* (è difficile trovare due messaggi diversi che producano lo stesso valore *hash*).
