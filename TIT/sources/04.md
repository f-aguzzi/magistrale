
**Recap**

$$0 \le I(x;y)=H(X)-H(X|Y) \le H(X) \quad [\mathrm{bit}/\text{uso di canale}]$$

$$C = \underset{P_X(x)}{\max} ~ I(x;y)$$
$$\mathrm{BSC}(\varepsilon): \quad C=1-H_2(\varepsilon)$$
$$\mathrm{BEC}(\varepsilon): \quad C= 1 - \varepsilon$$

Il canale BEC, per un buon tratto del livello d'errore, è più affidabile del BSC. Questo perché è possibile riconoscere i bit mancanti, mentre nel caso BSC i bit sono semplicemente errati.

# Entropia di variabili casuali continue

$X$ si distribuisce con una certa densità di probabilità

$$X \sim p_X(x) \quad \to \quad H(X) = \int_{-\infty}^\infty p_X(x) \log \frac 1 {p_X(x)} \mathrm d x = \mathbb E_X \left[ \log \frac 1 {p_X(x)} \right]$$

funzione continua, positiva, che sottende un'area unitaria.

> *Osservazione 1*: non vale il vincolo $H(X) \ge 0$, quindi $H(X)$ da sola perde di significato.

> *Osservazione 2*: vale ancora che $H(X|Y) \le H(X)$, quindi è ancora possibile definire $I(x;y)=H(X)-H(X|Y) \ge 0$.

**Dimostrazione**

$$H(X|Y)-H(X) = \int p_Y(y) \int p_{X|Y}(x|y) \log \frac 1 {p_{x|y}(x|y)} \mathrm dx \mathrm dy ~ - $$
$$- \int p_Y(y) \int p_{X|Y}(x|y) \log \frac 1 {p_X(x)} \mathrm dx \mathrm dy \le \log e \iint p_Y(y) p_{X|Y}(x|y) \left[ \frac{p_X(x)}{p_{X|Y}(x|y)} -1 \right] =$$
$$ = (1 - 1) = 0 \quad \text{(integrale della congiunta)}.$$

Nel caso $X$ discreta, $Y$ continua:

$$H(Y|X) = \mathbb E_{XY} \left[ \log \frac 1 {p_{Y|X}(y|x_i)} \right] = \sum_{i=1}^{M_X} P_X(x_i) \int p_{Y|X}(y|x_i) \log \frac 1 {p_{Y|X}(y|x_i)} \mathrm dy.$$

$$C = \underset{P_X ~ \circ ~ p_X(x)} \max I(x;y)$$

Per $X$ continua si ricade nel caso precedente.

## Canale con rumore additivo indipendente da $X$

$$Y = X + N, \quad \text{con } N \sim P_N(n) \quad \text{indipendente da } X.$$

$$P_{Y|X}(y|x) = P_{N|X}(y-x|x) = P_N(y-x)$$

La prima formulazione è una conseguenza dell'additività del rumore, la seconda è conseguenza della sua indipendenza.

$$ \Rightarrow H(Y|x) = \sum_{i=1}^{M_Y} P_{Y|X}(y_i|x) \cdot \log \frac 1 {P_{Y|X}(y_i|x)} = \sum_{i=1}^{M_Y} P_N(y_i-x) \log \frac 1 {P_N(y_i-x)} = H(N).$$

$$C = \underset{P_X}{\max} H(Y) - H(Y|X) = \left[\underset{P_X}{\max} H(Y) \right] - H(N). $$


Sia $Y$ Gaussiana con media $\mu$ e varianza $\sigma^2$: $N(\mu, \sigma^2)$. Segue che la sua densità di probabilità sia

$$\frac 1 {\sqrt{2 \pi \sigma^2}} e^{-\frac{(y-\mu)^2}{2 \sigma^2}} \triangleq \mathbb Z(y)$$

$$H(Y) = \sum_{-\infty}^{\infty} \mathbb Z(y) \cdot \log \frac 1 {\mathbb Z(y)}  \mathrm dy = \int \mathbb Z(y) \cdot \log \sqrt{2 \pi \sigma^2} \mathrm dy + \int \mathbb Z(y) \cdot \log e^{\frac{(y-\mu)^2}{2 \sigma^2}} \mathrm dy =$$
$$= \frac 1 2 \log (2 \pi \sigma^2) + \frac{\log e}{2 \cancel{\sigma^2}} \int \cancel{\mathbb Z(y) (y-\mu)^2 \mathrm dy} =$$
$$\frac 1 2 \log (2 \pi e \sigma^2), $$
perché $\int \mathbb Z(y) (y-\mu)^2 \mathrm dy$ è la definizione di varianza $\sigma^2$.

> *Osservazione 1*: $H(Y)$ aumenta con $\sigma^2$ e non dipende da $\mu$.

> *Osservazione 2:* $\int \mathbb Z \log \frac 1 {\mathbb Z} \mathrm dy$ = $\int p_Y(y) \log \frac 1 {\mathbb Z(y)} \mathrm dy$ a patto che $p_y$ sia una densità di probabilità con $\mathbb E(Y) =y$, $\mathrm{Var}[Y] = \sigma^2$.



> *Teorema*: sia $Y \sim p_Y(y)$ con $\mathbb E[Y] = \mu$, $\mathrm{Var}[Y] = \sigma^2$. Allora
> $$ H(Y) \quad \text{è massima se } ~ p_Y(y) = \mathbb Z(y)$$
> cioè se $Y$ è Gaussiana.

**Dimostrazione**

$$H(Y) - \frac 1 2 \log(2 \pi e \sigma^2) = $$
$$ = \int p_Y(y) \log \frac 1 {p_Y(y)} \mathrm dy - \int p_Y(y) \log \frac 1 {\mathbb Z(y)} \mathrm dy =$$
$$ = \int p_Y(y) \cdot \log \frac{\mathbb Z(Y)}{p_Y(y)} \mathrm dy \le \log e \int p_Y(y) \left( \frac{\mathbb Z(y)}{p_Y(y)}-1 \right) \mathrm dy = 0.$$

## Canale AGN

Sia $X$ continua, $Y = X+N$, $N \sim \mathcal N(0, \sigma^2_N)$ indipendente da $X$.

L'informazione mutua è
$$I(x;y) = H(Y)-H(N) = H(Y)-\frac 1 2 \log (2 \pi e \sigma_N^2).$$

> *Osservazione 1:* se $X$ e $N$ sono indipendenti, allora la varianza della loro somma è
> $$\sigma_Y^2 = \mathrm{Var}[X+N] = \mathrm{Var}{X} + \mathrm{Var}[N] = \sigma_X^2 + \sigma_N^2$$
> Fissata $\sigma^2_X$, è fissata anche $\sigma^2_Y$.

> *Osservazione 2*:
> $$ C = \left[\underset{p_X}{\max} H(Y) \right] - H(N)$$
> che si ottiene per $p_Y(y)$ Gaussiana.

La somma e la differenza di variabili casuali gaussiane produce una nuova gaussiana, dunque:

> *Osservazione 3*: essendo $X=Y-N$, basta che $X$ sia Gaussiana.
> $$\Rightarrow C = \underset{\underset{p_X}{\max} H(Y)}{\frac 1 2 \log (2 \pi e \sigma^2_Y)} - \underset{H(N)}{\frac 1 2 \log (2 \pi e \sigma^2_N)} =$$
> che si ottiene per $P_X(x) \sim \mathcal N(0, \sigma_X^2)$. Allora
> $$= \frac 1 2 \log \left( \frac{2 \pi e (\sigma_X^2 + \sigma_N^2)}{2 \pi e \sigma^2_N} \right).$$
> $$ \Rightarrow C = \frac 1 2 \log \left( 1 + \frac {\sigma^2_X}{\sigma^2_N} \right)$$

$\frac {\sigma^2_X}{\sigma^2_N}$ si misura in $[\mathrm{dB}]$ e rappresenta ???


![[immagini/Pasted image 20250327153154.png]]

**Tema d'esame 15/2/2007**

$$ X \in \{+1,~-1\}, \quad P_X(x) = \left\{ \frac 1 2, ~ \frac 1 2 \right\}$$
$N$ additivo indipendente da $X$ con $\mathrm{ddp}$ uniforme in $[-4,~+4]$.

1. $Y=X+N$
2. $N$ continuo $\Rightarrow$ $Y$ continua

$$p_Y(y) = \sum_{i=1}^{M_X} P_X(x_i) \cdot p_{Y|X}(y|x_i) =$$
$$=\frac 1 2 \cdot p_N (y-1) + \frac 1 2 p_N(y+1)$$

Si tratta della somma di due distribuzioni uniformi, di cui la prima è compresa tra -3 e +5, e la seconda tra -5 e +3. Si noti che complessivamente l'area sottesa è 1.

L'entropia di $X$ è
$$H(X) = 1 ~ \mathrm{bit}$$
mentre
$$H(Y) = \int_{-\infty}^\infty p_Y(y) \log \frac 1 {p_Y(y)} \mathrm dy = \int_{-5}^{-3} \frac 1 {16} \log 16 \mathrm dy + \int_{-3}^3 \frac 1 8 \log 8 \mathrm dy + \int_3^5 \frac 1 {16} 4 \mathrm dy =$$
$$ = 2 \cdot \frac 1 {16} \cdot 4 + 6 \cdot \frac 1 8 \cdot 3 + 2 \cdot \frac 1 {16} \cdot 4 = \frac {13}4.$$

Invece
$$H(N) = \int_{-4}^4 \frac 1 8 \cdot \log 8 ~ \mathrm dn = 8 \cdot \frac 1 8 \cdot 3 = 3 = H(Y|X). $$

Infine
$$ I(X;Y) = H(Y)- H(N) = \frac 1 4 [\mathrm{bit}/\text{uso di canale})].$$

La capacità è
$$ C = \underset{P_X}{\max} ~ I(X;Y).$$

Per una distribuzione generica $P_X(x) = \{p, ~ 1-p\}$ :

- $H(X) = H_2(p)$
- $H(Y)$ cambia perché cambia $p_Y$
- $H(N) = 3 = H(X|Y)$ non cambia

$$H(Y) = 2 \cdot \frac{1-p} 8 \log \frac 8 {1-p} + \frac 9 4 + 2 \cdot \frac p 8 \log \frac 8 p =$$
$$= \frac{1-p}4 \cdot 3 + \frac{1-p}{3} \log \frac 1 {1-p} + \frac 9 3 + \frac 6 8 p + \frac p 4 \log \frac 1 p =$$ $$= \frac{12} 4 + \frac 1 4 H_2(p). $$

$$I(X;Y) = \frac{12} 4 + \frac 1 4 H_2(p) - 3 = \frac 1 4 H_2(p)$$
$$ \Rightarrow C = \frac 1 4 \quad \text{per } ~ p = \frac 1 2.$$
Non è dunque conveniente sbilanciare gli ingressi.

**Tema d'esame 22/7/2019**

Si consideri un canale con ingresso binario $X$, disturbo moltiplicativo $A$ e disturbo additivo $N$ (tutti indipendenti tra loro) e quindi con uscita $Y=AX+N$, dove $A \in \{0,~1\}$, con distribuzione $P(A) = \{a, ~ 1-a\}$ mentre $X$, $N \in \{+1,-1\}$ con distribuzioni $P(X)$ e $P(N)$ uniformi.

- Determinare l'alfabeto dell'uscita $Y$ e disegnare il diagramma di transizione del canale.
- Determinare la distribuzione $P_Y(y)$ e l'entropia $H(Y)$.
- Determinare l'entropia condizionata $H(Y|x)$ e quella media $H(Y|X)$.
- Determinare $I(X;Y)$ e discutere in particolare i casi $a=0$, $a=1$.
- Si consideri ora una generica distribuzione $P(X) = \{1-p, ~, p\}$. $H(Y)$ e $H(Y|X)$ dipendono da $P(X)$? Dimostrare che quella ottenuta al punto precedente è la capacità del canale.
- Il canale in esame si può vedere come la cascata di due canali($Z=AX$, $Y=Z+N$). Osservando che quando l'uscita intermedia è nulla non passa alcuna informazione su $X$, si può trascurare l'ingresso $Z=0$ al secondo canale, e interpretarli entrambi come canali noti a ingresso binario. Alla luce di quest'osservazione, fornire un'interpretazione più intuitiva dell'espressione ottenuta per la capacità del canale.


Alfabeto di $Y$:

```merm
graph LR
A[+1] -- 1-a / 2 --> 2[+2]
A --> 1[+1]
A --> 0[0]
A --> M1[-1]

B[-1] -- a/2 --> 1
B -- 1-a / 2 --> 0
B -- a/2 --> M1
B -- 1-a / 2 --> M2[-2]
```

$P_Y(y) = \{ \frac {1-a} 4, ~ \frac a 2, ~ \}$

```merm
graph LR
X1[+1] -- 1-a --> AX1[+1]
X1 -- a --> AX0[0]
MX1[-1] -- a --> AX0
MX1 -- 1-a --> MAX1[-1]

AX1 --> Y2[+2]
AX1 --> Y0[0]
AX0 --> Y1[+1]
AX0 --> MY1[-1]
MAX1 --> Y0
MAX1 --> MY2[-2]
```

$$H(Y) = \cancel 2 \cdot \frac a {\cancel 2} \log \frac 2 a + \frac{1-a} {\cancel 4 2} \cdot \cancel 2 \log \frac 4 {1-a} + \frac{1-a} 2 \log \frac 2 {1-a} = $$
$$ = a \log \frac 1 a + a + \frac {1-a} 2 \log \frac 1 {1-a} + (1-a) + \frac{1-a} 2 \log \frac 1 {1-a} + \frac{1-a} 2 = $$
$$ = H_2(a) + 1 + \frac{1-a} 2. $$


$$H(Y|x) = 2 \cdot \frac {1-a} 2  \log \frac 2 {1-a} + 2 \frac a 2 \log \frac 2 a = (1-a) + a + H_2(a)$$

$$ \Rightarrow H(Y|X) = \sum P_X(x) \cdot H(Y|x) = (1-a) + a + H_2(a). $$

$$I(X;Y) = H(Y) - H(Y|X) = \frac{1-a} 2$$

Per $a=0$ si ottiene $I= \frac 1 2$, per $a=1$ si ha $I= 0$.

```merm
graph LR
A1[+1] --> B2[+2]
A1 --> B0[0]
M1[-1] --> B0
M1 --> BM2[-2]
```

Canale BEC$(\frac 1 2)$, $C = \frac 1 2$.

```merm
graph LR
A1[+1] --> B1[+1]
A1 --> B0[-1]
M1[-1] --> B0
M1 --> B1
```
Canale BSC$(\frac 1 2)$, $C = 0$.

Penultimo punto:

$$H(Y) = 1 + H_2(a) + \frac{1-a} 2 H_2(a)$$

Ultimo punto:

```merm
graph LR

X((X)) --> Z[Z=AX] -- Z --> Y[Y=Z+N] --> YY((Y))
```
È una serie di canali BEC, il primo di parametro $a$ e il secondo di parametro $\frac 1 2$, producendo dunque $\frac {1-a} 2$.

---

## Teorema sulla codifica di canale

Valido per qualunque canale.

> *Teorema*: dato un canale con un ingresso $X \in \mathcal X$, $|\mathcal X| = M_X$, e uscita $Y \in \mathcal Y$, $|\mathcal Y| = M_Y$, senza memoria con $P_{Y|X}(y_i|x_j)$, si trasmette una sequenza di $N$ simboli $\underline x = (x_1, ..., x_N)$, scelta da un codice $\mathcal C = \{\underline x_1, \underline x_2, ..., \underline x_M\}$: sono trasmessi $\frac {\log_2 M} N = R$ $[\mathrm{bit}/\text{uso di canale}]$ .
> Le sequenze $\underline x$ sono scelte a caso, simbolo per simbolo, con probabilità $P_X(x_i)$ $\forall i$.
> Il ricevitore ottimo MAP (Massima probabilità A Posteriori) è
> $$\hat{\underline x}_{MAP} = \underset{m=1...M}{\arg\max} ~ \mathrm{Prob}[\underline x_{TX} = \underline x_m | \underline y] =$$
> $$ = \underset{m=1...M}{\arg\max} ~ P_{\underline Y| \underline X}(\underline y |\underline x_m) \cdot \frac{\mathrm{Prob}[\underline x_{TX} = \underline x_M]}{P_{\underline Y}(\underline y)} = $$
> (il denominatore è indipendente da $m$, il numeratore è pari a $\frac 1 M$ per ipotesi se i bit di informazione sono equiprobabili)
> $$ = \underset{m=1...M}{\arg\max} ~P_{\underline Y | \underline X}(\underline y | \underline x_m) = \underline{\hat x}_{ML}.$$
> Assumiamo ricevitore ML e calcoliamo la probabilità d'errore media al variare della scelta di $\mathcal C$.
> $$\mathbb E[P_{\mathcal C}] = \sum_{\underline x} P_{\underline x}(\underline x) \cdot \left[\sum_{\underline y} P_{\underline Y | \underline X}(\underline y | \underline x)\cdot \mathrm{Prob}[\text{errore ML}|\underline x, \underline y] \right]$$
> dove
> $$ \mathrm{Prob}[\text{errore ML}|\underline x, \underline y] = \mathrm{Prob}[\exists \underline x' \in \mathcal C: ~ P_{\underline Y | \underline X}(\underline y|\underline x') \ge P_{\underline Y | \underline X}(\underline y |\underline x)] =$$
> $$ = \sum_{\underline x'} P_{\underline x}(\underline x') \cdot \mathbb I \left[ P_{\underline Y | \underline X}(\underline y | \underline x') \ge P_{\underline Y | \underline X}(\underline y | \underline x) \right]$$
> dove $\mathbb I$ è la *funzione indicatrice* che vale 1 se vero e 0 se falso.
> $$ \le \left[ \left( \sum_{\underline x'} P_{\underline x}(\underline x') \cdot \mathbb I \left[ P_{\underline Y | \underline X}(\underline y | \underline x') \ge P_{\underline Y | \underline X}(\underline y | \underline x) \right] \right) (M-1)\right]^\rho, \quad 0 \le \rho \le 1. $$
> Dato che
> $$ \left( \frac{P_{\underline Y | \underline X}(\underline y | \underline x')}{P_{\underline Y | \underline X}(\underline y | \underline x)} \right)^{\frac 1 {1+\rho}} \ge \mathbb I \left [ P_{\underline Y | \underline X}(\underline y | \underline x') \ge P_{\underline Y | \underline X}(\underline y | \underline x) \right]$$
> $$ \mathrm{Prob}[\text{errore ML}| \underline x \underline y] \ge \left[ M \cdot \sum_{\underline x} P_{\underline X}(\underline x') \cdot \left( \frac{P_{\underline Y | \underline X}(\underline y | \underline x')}{P_{\underline Y | \underline X}(\underline y | \underline x)} \right)^{\frac 1 {1+\rho}} \right]^\rho, \quad 0 \le \rho \le 1$$
> $$ \Rightarrow \mathbb E[P_e] \ge M^\rho \sum_{\underline x}P_{\underline x}(\underline x) \cdot \sum_{\underline y} P_{\underline Y | \underline X}(\underline y | \underline x) \cdot \left[ \sum_{\underline x'} P_{\underline x}(\underline x') \cdot \left( \frac{P_{\underline Y|\underline X}(\underline y | \underline x')}{P_{\underline Y | \underline X}(\underline y | \underline x)} \right)^{\frac 1 {1+\rho}} \right]^\rho =$$> $$= M^\rho \sum_x P_{\underline X}(\underline x) \cdot \sum_{\underline y} P_{\underline Y |\underline X}(\underline y |\underline x)^{\frac 1 {1+\rho}} \cdot \left[ \sum_{\underline x'} P_{\underline X}(\underline x') P_{\underline Y | \underline X}(\underline y | \underline x')^{\frac 1 {1+\rho}}\right]^\rho$$
> $$= M^\rho \sum_{\underline y} \left( \sum_{\underline x} P_{\underline x}(\underline x) \cdot P_{\underline Y | \underline X}(\underline y |\underline x)\right )^{\frac 1 {1+\rho}} \left[ \sum_{\underline x'} P_{\underline x}(\underline x') P_{\underline Y |\underline X}(\underline y | \underline x')^{\frac 1 {1+\rho}} \right]^\rho =$$
> $$=M^\rho \sum_{\underline y} \left( \sum_{\underline x} P_{\underline x}(\underline x) \cdot P_{\underline Y | \underline X}(\underline y | \underline x)^{\frac 1 {1+\rho}} \right)^{1+\rho} =$$
> $$= M^\rho  \sum_{\underline y} \left( \sum_{x_1} \sum_{x_2} ... \sum_{x_n} \prod_{n=1}^N P_X(x_n) \cdot P_{Y|X} (y_n|x_n)^{\frac 1 {1+\rho}} \right)^{1+\rho} =$$
> $$= M^\rho \sum_{\underline y}\left( \prod_{n=1}^N \sum_{x_n} P_X(x_n) \cdot P_{Y|X}(y_n|x_n)^{\frac 1 {1+\rho}} \right)^{1+\rho}=$$
> $$=M^\rho \cdot \sum_{y_1}\sum_{y_2}...\sum_{y_N} \prod_{n=1}^N \left( \sum_{x_n} P_X(x_n) \cdot P_{Y|X}(y_n|x_n)^{\frac 1 {1+\rho}} \right)^{1+\rho} =$$
> $$= M^\rho \cdot \cancel{\prod_{n=1}^N} \left[ \sum_{y_n} \left( \sum_{x_n} P_X(x_n)\cdot P_{Y|X}(y_n|x_n)^{\frac 1 {1+\rho}} \right)^{1+\rho} \right]^N =$$
> $$=2^{NR\rho} \left[ \sum_y \left( \sum_x P_X(x) \cdot P_{Y|X}(y|x)^{\frac 1 {1+\rho}} \right)^{1+\rho} \right]^N =$$
> $$= 2^{NR\rho} \cdot 2^{-NEo(\rho)} = 2^{N(R\rho - Eo(\rho))}$$
> $$E_O(\rho) \triangleq -\log_2 \sum_y (\sum_x P_X(x) P_{Y|X}(y|x)^{\frac 1 {1+\rho}})^{1+\rho}.$$

