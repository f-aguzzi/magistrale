
**Tema d'esame del 17/7/2014**

Si consideri il codice $RS(7,5)$ primitivo non accorciato con $J_0 = 1$. Trasmessa una parola, si riceve $v_6 = c_6 + e_6$ corrotto dall'errore $e_6 = \alpha^2$. 

1. In che campo è progettato il codice? Qual è $t$?

$N=7$, $k = 5$.

Dato che $N=7$ e il campo è primitivo, $q = N+1 = 8$ $\Rightarrow$ $GF(8) = GF(2^3)$ generato da $p(\alpha) = \alpha^3 + \alpha + 1$.

Osservazione: $\underline C = [c_0 ~ c_1 ~ c_2 ~ ... ~ c_6]$, $c_i \in GF(8)$, $C_j \in GF(8)$ (Reed-Solomon).

$N-k=2$ $\to$ grado di $g(x)$ = 2.

Abbiamo 2 zeri consecutivi nella trasformata $\underline C$.

Se il polinomio ha grado 2, allora è il prodotto di due monomi (i due zeri):

$$ C_{j_0}, ~ C_{j_0+1}$$
$$ C_1 = C_2 = 0$$

Il polinomio generatore è $g(x) = (x-\alpha)(x-\alpha^2) = x^2 + (\alpha+\alpha^2)x + (\alpha+1)$.

Dalla presenza dei due zeri consecutivi, ricaviamo che la distanza di Hamming è $N-k+1 \to d=3$, con potere correttore $t=1$.

2. Calcolare $\Lambda(x)$ (polinomio locatore).

Ci si aspetta che $\Lambda(x)$ abbia una radice in $\alpha^{-6}$, dato che l'errore è $e_6$.

$\Lambda(x) = 1-\alpha^6 x$ perché $\mathrm{grado}\left( \Lambda(x) \right) = 1$ e $\Lambda(\alpha^{-6}) = 0$.

Alternativamente, si possono calcolare le $d-1 = 2$ sindromi:

$$
\begin{matrix}
S_0 = E_1 = \sum_{i=0}^{n-1 = 6} e_i \alpha^{1 \cdot i} = e_6 \cdot \alpha^6 = \alpha^2 \cdot \alpha^6 = \alpha^8 = \alpha \\
S_1 =E_2 = \sum_{i=0}^6 e_i \alpha^{2i} = \alpha^2 \cdot \alpha^{12} = 1
\end{matrix}
$$

Key equation:
$$ S_0 \Lambda_1 + S_1 = 0 \Rightarrow \Lambda_1 = \frac{S_1}{S_0} = \alpha^{-1} = \alpha^6$$

3. Calcolare il polinomio valutatore degli errori $\Omega(x)$ e valutarne la coerenza con $e_6$.

$$\Omega(x) = S(x) \Lambda(x) \mod x^2 \quad (\text{grado } \nu-1 = 0) $$

La dicitura $\mod x^2$ significa che bisogna scartare i termini di grado superiore al primo.

$$ = (S_0 + S_1 x)(\Lambda_0 + \Lambda_1 x) \mod x^2 =$$
$$ = \alpha + \underbrace{\cancel{(\alpha^7 + 1)x}}_{\Lambda_0 S_1 + \Lambda_1 S_0 = 0} + \alpha^6 x^2 \mod x^2 = \alpha $$

La verifica avviene controllando che il valore dell'errore corrisponda alla seguente espressione basata su $\Omega(x)$ e la derivata di $\Lambda(x)$:
$$e_{i_n} = -\frac{\Omega(\alpha^{-in})}{\Lambda'(\alpha^{-in})}$$
dove in questo caso $\Lambda'(x) = \Lambda_1 = \alpha^6$.

Calcoliamo

$$ e_6 = - \frac{\alpha}{\alpha^6} = \alpha^{-5} = \alpha^2 $$
che è il valore effettivo dell'errore.

4. In quanti passi converge l'algoritmo di Euclide?

L'algoritmo sostituisce la key equation e si basa sulla definizione $\Omega(x) = S(x) \Lambda(x) + \Phi(x) x^2$ dove $\Phi(x)$ è fatto in modo da annullare i termini di grado superiore al primo.

I termini incogniti sono $\Omega$, $\Lambda$ e $\Phi$.

$$
\begin{cases}
\Omega^{(-1)}(x) = x^2 \quad \text{grado 2} \\
\Lambda^{(-1)}(x) = 0 \quad \text{grado 0} \\
\Phi^{(-1)}(x) = 1
\end{cases}

\quad

\begin{cases}
\Omega^{(0)}(x) = S(x) \quad \text{grado 1} \\
\Lambda^{(0)}(x) = 1 \quad \text{grado 0} \\
\Phi^{(0)}(x) = 0
\end{cases}
$$

$$ q^{(1)}(x) = \frac{\Omega^{(-1)}(x)}{\Omega^{(0)}(x)} = $$
Divisione:

| $x^2$<br>$x^2+\alpha x$           | $\alpha+x$        |
| --------------------------------- | ----------------- |
| $\alpha x$<br>$\alpha^2+\alpha x$ | $x+\alpha = q(x)$ |
| $r(x) = \alpha^2$                 |                   |
$q^{(1)}(x) = x+\alpha$
$r^{(1)}(x) = \alpha^2$

$$
\begin{cases}
\Omega^{(1)}(x) = r^{(1)}(x) = \alpha^2 \quad \text{grado 0} \\
\Lambda^{(1)}(x) = \Lambda^{(-1)}(x) + q^{(1)}(x) \Lambda^{(0)}(x) = x+\alpha \\
\Phi^{(1)}(x) = \Phi^{(-1)}(x) + q^{(1)}(x) \Phi^{(0)}(x) = 1 \quad \text{grado 1}
\end{cases}
$$

L'algoritmo dunque converge in un passo.

*Osservazione:* l'algoritmo di Euclide trova $\Lambda(x) = \alpha + x$ e $\Omega(x) = \alpha^2$, mentre la key equation trova $\tilde \Lambda(x) = 1 + \alpha^6 x$ e \tilde $\Omega(x) = \alpha$.
Entrambe le soluzioni sono corrette e trovano lo stesso valore di $e_i$. La key equation trova un $\Lambda$ con $\Lambda_0 = 1$. La relazione che le collega è:

$$ \Lambda(x) = \alpha \tilde \Lambda(x) $$
$$ \Omega(x) = \alpha \tilde \Omega(x)$$

Le radici non cambiano e neppure il rapporto
$$\frac{\Omega(x)}{\Lambda'(x)} = \frac{\tilde \Omega(x)}{\tilde \Lambda'(x)}. $$

# Codici LDPC per canali BEC e AGN

Canale $BEC(\varepsilon)$: $C = 1-\varepsilon$

```merm
graph LR
Xz(0) -- 1-e --> Yz(0)
Xu(1) -- 1-e --> Yu(1)
Xz(0) -- e --> Yu(1)
Xu(1) -- e --> Yz(0)
```

Sostituire i bit non arrivati con bit estratti casualmente peggiora le prestazioni del canale, perché fa perdere l'informazione sulla posizione degli errori.

$$
\begin{matrix}
\text{Blocchi grandi} \\
\text{Decodifica ML}
\end{matrix}

\Rightarrow

P_{\underline y | \underline x} (\underline y | \underline x) = \prod_{n=1}^N P_{y|x}(y_n | x_n) =
\begin{cases}
\varepsilon^{N_e}(1-\varepsilon)^{N-N_e} \quad \text{ se } \underline x \text{ è compatibile con } \underline y \\
0 \quad \text{ se } x_n \ne y_n \text{ per qualche } y_n \ne e
\end{cases}
$$
dove $N_e$ p il numero di cancellazioni $\approx \varepsilon n$.

Quindi il decodificatore ML deve solo trovare $\underline x$ compatibile con $\underline y$ e sperare che sia unica.

Sia $\underline x \in \mathcal C$: $\underline{\underline H} ~ \underline x = \underline 0$ con $\underline{\underline H}$ matrice di parità binaria $[N-k ~ \times N]$, $\underline x$ $[N \times 1]$.

A blocchi:

$$
\underline{\underline H}_{\underline e} \underline x_{\underline e} + 
\underline{\underline H}_{\underline{\overline e}} \underline x_{\underline{\overline e}} = \underline 0
$$

$$
\Rightarrow
\underbrace{\underline{\underline H}_{\underline e}}_{[N-k \times N_e]}
\underbrace{\underline x_{\underline e}}_{[N_e \times 1]}
=
\underbrace{\underline s}_{[N-k \times 1]} $$

dove $\underline e$ = insieme degli indici tali che $y_{\underline e} = \underline e$ e $\overline e$ è l'insieme complementare.

Per risolvere un sistema lineare in $GF(2)$:
1. eliminazione gaussiana (ottiene una matrice triangolare alta in $\underline{\underline H}_{\underline e}$ e porta a 1 tutti gli elementi di $\underline s$ tranne l'ultimo) con complessità $O(N^3)$
2. *back substitution* per trovare $\hat {\underline x_e} = \underline x_e$ con complessità $O(N^2)$

L'$O(N^3)$ deriva dall'assunzione che la matrice di parità sia piena. Se la matrice fosse riempita sparsamente ($k \perp N$ in ogni colonna, $j \perp N$ in ogni riga), si potrebbe sostituire l'eliminazione gaussiana con un'operazione lineare.

## Codici LDPC (Low Density Parity Check)

Nella matrice di parità ci sono $j$ valori a 1 in ogni colonna e $k$ valori a 1 in ogni riga, con $j$ e $k$ indipendenti da $N$. Se $j$ e $k$ sono costanti per ogni colonna e riga, si parla di codice LDPC *regolare*.

Se nella matrice $H [m \times n]$ le righe sono tutte indipendenti, allora $m = N-k$ e quindi il rate del codice è $R = 1- \frac m N = 1 - \frac j k$.

*Esempio*:

$$
\underline{\underline H} =
\begin{bmatrix}
1 & 0 & 0 & 1 & 1 & 0 \\
1 & 1 & 0 & 0 & 0 & 1 \\
0 & 1 & 1 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 & 0 & 1
\end{bmatrix}
$$
con $m' = 3$ righe indipendenti, $j=2$ e $k=3$. Il rate è dunque $R = -\frac{m'} N = 1 - \frac 3 6 = \frac 1 2$.

La riduzione gaussiana assicura che ci sia sempre almeno un bit da cui partire per la decodifica. In questo caso non è necessaria perché ci si può aspettare di essere in questa situazione senza dover applicare alcuna operazione alla matrice di parità.

$\underline y = (0,e,e,e,0,0)$

Le posizioni dei tre errori indicano che le colonne da considerare nella matrice di parità siano la seconda, la terza e la quarta ($H_{\overline e}$).

$$
H_{\overline e} \cdot x_{\overline e} =
\begin{bmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
???
\end{bmatrix}
$$

Grafo di Tanner, stato iniziale:

```merm
graph LR
x1((x1))
x2((x2))
x3((x3))
x4((x4))
x5((x5))
x6((x6))

c1[c1]
c2[c2]
c3[c3]
c4[c4]

x1 --> c1
x1 --> c2
x2 --> c2
x2 --> c3
x3 --> c3
x3 --> c4
x4 --> c1
x4 --> c4
x5 --> c1
x5 --> c3
x6 --> c2
x6 --> c4
```

Iterazione:
1. VN (*Variable Nod*e) con $y_x \ne e$ lo comunicano ai CN (*Check Node*) vicini (connessi);
2. i CN accumulano i valori ricevuti. Se ricevono $k-1$ valori diversi da $e$ possono calcolare il valore dell'ultimo e inviarglielo (la somma di tutti i bit ricevuti deve fare 1);
3. i nodi che hanno inviato l'informazione possono spegnersi. I loro spigoli sul grafo possono essere cancellati.

Ogni VN invia $j$ valori e ogni CN ne riceve $k$.

Dopo la prima iterazione:
```merm
graph LR
x1((x1))
x2((x2))
x3((x3))
x4((x4))
x5((x5))
x6((x6))

c1[c1]
c2[c2]
c3[c3]
c4[c4]

x2 --- c2
x2 --- c3
x3 --- c3
x3 --- c4
x4 --- c1
x4 --- c4
```

Iterazione successiva:

```merm
graph LR
x1((x1))
x2((x2))
x3((x3))
x4((x4))
x5((x5))
x6((x6))

c1[c1]
c2[c2]
c3[c3]
c4[c4]

x2 --- c3
x3 --- c3
x3 --- c4
x4 --- c4
```

I nodi $c1$ e $c2$ hanno terminato il loro lavoro. I bit di $x_2$, $x_3$ e $x_4$ possono essere ricostruiti. La sequenza completa ricostruita è $(000000)$.

## Convergenza della decodifica iterativa

```merm
graph LR
1-epsilon --> A((.))

A --> c1[c1]
A --> c2[c2]
A --> c3[c3]

c21((.)) --> c2
c22((.)) --> c2
c23((.)) --> c2

c31((.)) --> c3
c32((.)) --> c3
c33((.)) --> c3

1(1-epsilon) --> c21
2(1-epsilon) --> c22
3(1-epsilon) --> c23

4(1-epsilon) --> c31
5(1-epsilon) --> c32
6(1-epsilon) --> c33
```


$$ 1- \varepsilon (1-(1-\varepsilon)^{k-1})^j = P(x_k \ne e) $$ 
Se $N \to \infty$, il grado tende ad un albero per qualsiasi iterazione.

RIPORTARE LA CATENA

Ad ogni iterazione, un VN riceve:
- $1-\varepsilon$ dal canale all'inizio
- $1-q$ da ciascuno dei $j-1$ check node a cui è collegato
e invia $1-p$.

$$ p = \varepsilon \cdot q^{j-1}$$
(probabilità che non riesca a trasmettere la propria informazione e non la riceva). Per avere la probabilità di propagazione, bisogna calcolare il complemento a 1:
$$ 1-p = 1-\varepsilon \cdot q^{j-1} = I^{(V)} $$

I CN ricevono $1-p$ dai $k-1$ VN a cui sono connessi, e trasmettono $1-q$. In questo caso dunque l'informazione in uscita è
$$ 1-q = (1-p)^{k-1} = I^{(C)} $$

EXIT Chart (EXtrinsic InformaTion Chart, grafico di trasferimento dell'informazione estrinseca):

Asse $y$: $I^{(V)}$, da 0 a 1
Asse $x$: $I_a$, da 0 a 1

$I^{(V)} = 1 - \varepsilon \cdot (1-I_a)^{j-1}$, dove $q = 1 - I_a$

Per $j$ crescente, sul grafico dei variable node il grafico tende sempre più allo spigolo in alto a sinistra. La curva parte da $1-\varepsilon$ sull'asse $y$ e arriva a (1,1).

Per i check node:

Asse $y$: $I^{(C)}$, da 0 a 1
Asse $x$: $I_a$, da 0 a 1

$I^{(C)} = (1-p)^{k-1} = I_a^{k-1}$. Curve che partono da (0,0), arrivano a (1,1) e, all'aumentare di $k$, aumentano d'ordine tendendo sempre più verso l'angolo in basso a destra.

Grafico combinato: ??? Variable node curva diretta, check node curva inversa per rappresentare le due I_A sullo stesso grafico

Se le curve si intersecano, si trova un punto di strozzatura che ferma l'aumento di informazione e impedisce di arrivare a 1, impedendo di identificare tutti i bit.

La decodifica iterativa converge se c'è il tunnel aperto tra le curve EXIT. $\varepsilon_T$ ($T$ = threshold, soglia) è il valore soglia per cui le curve si toccano.

Fissato $R = \frac j k$, cerco $j$ e $k$ che massimizzano $\varepsilon_T$. Per esempio, con $R = \frac 1 2$, la scelta migliore è $j=3, k=6$ con cui $\varepsilon_T = 0.43$, a cui corrisponde $C = 1-\varepsilon_T = 0.57$ a fronte di un $R = 0.5$. Questo è il costo della decodifica LDPC.

## Codici LDPC irregolari

Sistemare questo grafico:
```merm
graph LR
x1((x1))
x2((x2))
x3((x3))
x4((x4))

c1[c1]
c2[c2]
c3[c3]
c4[c4]

x1 --> c1
x1 --> c2
x2 --> c2
x2 --> c3
x2 --> c4
x3 --> c3
x4 --> c1
x4 --> c2
x4 --> c3
x4 --> c4
```

Il blocco centrale prende il nome di $\pi$.

Numero variabile di connessioni per ogni VN e CN. Misure:

- $\lambda_i$ ($\lambda$ = l = left) = frazione di rami (totale $E$) connessi a VN di grado $i$ (cioè con i rami connessi)
- $\rho_i$ ($\rho$ = r = right) = frazione di rami (totale $E$) connessi a CN di grado $i$

*Osservazioni:
1. un LDPC regolare con $j=3$, $k=6$ avrebbe $\lambda_3 =1$, $\rho_6 =1$ e $\lambda_i = \rho_i = 0$ per tutti gli altri.
2. $\sum_{i=1}^{D_r} \lambda_i = 1$, $\sum_{i=1}^{D_c} \rho_i =1$, $D_v$ e $D_c$ gradi massimi di VN e CN

Resta vero che $R = 1 - \frac m N = \frac{N-m}N$.

$\lambda_i \cdot E$ è il numero di spigoli entranti in un variable node quindi $\frac{\lambda_i \cdot E} i$ è il numero di variable node di grado $i$. Il numero di variable node totali è dunque
$$N = \sum_{i=0}^{D_r} \frac{\lambda_i \cdot E} i = E \int_0^1 \lambda(x) \mathrm d x, \quad \lambda(x) = \sum_{i=0}^{D_v} \lambda_i x^{i-1}$$
Inoltre
$$m = \sum_{i=0}^{D_C} \frac{E \cdot \rho_i}{i} = E \int_0^1 \rho(x) \mathrm dx, \quad \rho (x) = \sum_{i=1}^{D_c} \rho_i x^{i-1}$$

Al contrario dei codici regolari, i VN e CN non emettono necessariamente tutti la stessa quantità di informazione.

Informazione uscente da un VN di grado $i$:
$$ 1-\varepsilon(1-I_a)^{1-i} $$
(ma il grado è variabile da un nodo all'altro).

È possibile raggruppare queste informazioni in un'informazione media:

$$I^{(V)} = \sum_{i=1}^{D_v} \lambda_i \cdot (1-\varepsilon(1-I_a)^{i-1}) = $$
$$ =  1-\varepsilon \sum_{i=1}^{D_v} \lambda_i (1-I_a)^{i-1} =$$
$$ = 1-\varepsilon \lambda(x) |_{x = 1-I_a}.$$

Allo stesso modo si può calcolare $I^{(A)}$ per i CN:

$$I^{(C)} = \sum_{i=1}^{D_c} \rho_i I_a^{i-1} = \rho(x) |_{x=I_a}$$

Possiamo usare questi risultati nella EXIT chart, con una curva che rappresenta $1-\varepsilon \lambda (1-I_a^{(C)})$ per i variable node e una curva che rappresenti $\rho(I_a^{(V)})$ (invertita).

Nell'esempio sulle slide, si ottiene $\varepsilon_T = 0.47$ per $R = \frac 1 2$.

Date  $1-\varepsilon \lambda (1-I_a^{(C)})$ e $\rho(I_a^{(V)})$, è possibile calcolare l'area compresa tra le due curve. Dipende da $\varepsilon$ perché la curva relativa ai variable node è dipendente da $\varepsilon$. L'area è dunque calcolabile come:

$$A(\varepsilon) = \int_0^1 1 - \varepsilon \lambda(1-x \text{ oppure } x) \mathrm dx + \int_0^1 \rho(x) \mathrm d x - 1$$

perché la somma dell'area sopra la curva dei VN e l'area sottesa alla curva dei CN è pari all'area del quadrante (unitaria) più l'area compresa tra le due curve.

$$= \cancel 1 - \varepsilon \int_0^1 \lambda(x) \mathrm dx + \int_0^1 \rho(x) \mathrm d x - \cancel 1 = $$
$$ = \int_0^1 \lambda(x) \mathrm dx \left[ \frac{\int_0^1 \rho(x) \mathrm dx}{\int_0^1 \lambda(x) \mathrm dx} -\varepsilon \right] =  $$
$$ = \frac N E [1-\varepsilon - R]. $$

Se il rate è maggiore della capacità, le curve si intersecano e la decodifica iterativa non riesce a convergere. 

L'area è proporzionale tra la differenza tra la capacità e il rate. Tanto è più grande l'area, tanto meno il codice si avvicinerà alla capacità. Più è grande l'area nel caso in cui le due curve si tocchino in un solo punto (in prossimità della soglia), maggiore sarà la capacità persa.

La riduzione dell'area, però, aumenta il numero di iterazioni necessarie per la convergenza della decodifica iterativa.

I codici iterativi sono comunque *capacity achieving* se correttamente realizzati.

Per i canali gaussiani:

$$p_N(n) = N(0, \sigma^2) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{n^2}{2 \sigma^2} \right)$$

Due distribuzioni, $N(-1, \sigma^2)$ e $N(-1, \sigma^2)$ per rappresentare segnali binari.

LLR (rapporto di verosimiglianza):

$$ L = \ln \left( \frac{P(+1)}{P(-1)} \right) $$
dove
$$P(\pm 1) = \frac{e^{\pm \frac L 2}}{e^{\frac L 2} + e^{-\frac L 2}}$$

Per i VN la probabilità di uscita è il prodotto delle probabilità di ingresso; dato che si lavora con gli LLR, che sono logaritmici, questa operazione diventa una semplice somma.

L'equazione di parità (una XOR in binario) è dunque

$$\prod_{i=1}^{d_c} x_i = +1$$

