
**Riassunto sorgenti con memoria:**

$$H(X_k|X_{k-1}) \le H(X_k)$$

Non importa come siano legati i messaggi: se non sono indipendenti, l'entropia condizionata è minore di quella non condizionata.

Se invece è condizionata ad un messaggio specifico:

$$H(X_k|x_{k-1}) \lesseqgtr H(X_k)$$

Inoltre:

$$H(\mathcal X) = \lim_{L \to \infty} H(X_k|X_{k-1}...X_{k-L})$$

Se la sorgente ha memoria finita (memoria di $m$ messaggi):

$$H(\mathcal X) = H(X_k|X_{k-1}...X_{k-m})$$
ovvero la conoscenza di messaggi ad un orizzonte di più di $M$ indici indietro non ha alcun effetto sulla distribuzione del messaggio successivo.

Definizione alternativa:

$$H(\mathcal X) = \lim_{l \to \infty} \frac 1 L H(X_k|X_{k-1}...X_{k-L + 1})$$
> *Osservazione:* le due definizioni coincidono per ogni sorgente a memoria finita.

Entropia condizionata:
$$\frac 1 L H(\underline X) = \frac 1 L \left( H(x_{k-L+1}) + H(X_{k-L+2}|X_{k-L+1}) + H(X_{k-L+3}|X_{k-L+2}~X_{k-L+3}) + ... + H(X_k|X_{k-1}~X_{k-2}~X_{k-L+1}) \right)$$

L'ultimo termine è pari a

$$H(X_k|X_{k-1} ... X_{k-m})$$

Il condizionamento può solo ridurre l'entropia. Il contributo di ogni termine, andando indietro, è decrescente. I primi $m$ termini sono diversi tra loro, quelli da $m+1$ a $L$ sono uguali tra loro (e uguali all'$m$-esimo termine), quindi:

$$\frac 1 L (L-m) \cdot H(X_k | X_{k-1} ... X_{k-m}) + \frac 1 L \mathrm{cost(L)} \underset{L \to \infty}{\longrightarrow} H(X_k | X_{k-1} ... X_{k-m})$$
dove il termine costante è la somma di $m$ entropie con condizionamento a meno di $m$ messaggi precedenti, descritta sopra.

## Codifica Lempel-Ziv (LZ)

La codifica di Lempel-Ziv è una codifica universale per sorgenti binarie con memoria. Ne esistono diverse varianti; quella presentata di seguito è presa da *Information Theory, Inference and Learning Algorithms* di David McKay, docente di Cambridge.

**Esempio:** codificare LZ la sequenza $1011010100010$.

La distribuzione di probabilità stimata è circa $\frac 1 2$, $\frac 1 2$ ma è presente una memoria.

Passaggi:

-  *parsing*: suddividere la sequenza binaria in sottosequenze inedite:
   $$|1|0|11|01|010|00|10|$$
   in caso di sequenze già viste, le si codifica comunque in modo separato.
   Chiamiamo:
   - $s_1$: $1$
   - $s_2$: $0$
   - $s_3$: $11$ e così via.

- *codifica*: codificare $s_n$ come $(p_n, b_n)$ ovvero una coppia formata da un puntatore $p_n$ ad una sequenza già vista $s_i$ con $i < n$ e un bit inedito $b_n$.

| $n$         | $0$     | $1$              | $2$     | $3$      | $4$      | $5$       | $6$       | $7$       |
| ----------- | ------- | ---------------- | ------- | -------- | -------- | --------- | --------- | --------- |
| $s_n$       | $\cdot$ | $1$              | $0$     | $11$     | $01$     | $010$     | $00$      | $10$      |
| $(p_n,b_n)$ |         | $(\cancel 0, 1)$ | $(0,0)$ | $(01,1)$ | $(10,1)$ | $(100,0)$ | $(010,0)$ | $(001,0)$ |

*Osservazione:* Per codificare la sequenza $n$, serve un puntatore (binario) $p_n$ da $\lceil \log_2 n \rceil$ $\mathrm{bit}$.

Il primo $0$ si salta perché è ovvio.

La sequenza codificata è:
$$100011101100001000010$$

In questo caso la stringa codificata è più lunga dell'originale ma l'algoritmo, pur semplice, è asintoticamente ottimo.

La decodifica si ottiene separando la stringa in sottosequenze di lunghezza crescente in base alla dimensione dei puntatori ($\lceil \log_2 n \rceil$ $\mathrm{bit}$ + il bit inedito):

$$|1|00|011|101|1000|0100|0010|$$

da cui:

| $1$ | $00$ | $011$ | $101$ | $1000$ | $0100$ | $0010$ |
| --- | ---- | ----- | ----- | ------ | ------ | ------ |
| $1$ | $0$  | $11$  | $01$  | $010$  | $00$   | $10$   |

Una  buona codifica per sorgenti $\mathcal X$ con memoria deve generare una sequenza di bit con $\mathrm{pmf}$ uniforme (avere una pari probabilità per $0$ e $1$) ed essere senza memoria. Se così non fosse, sarebbe ulteriormente comprimibile per LZ.

**Esempio 1:** $\mathrm{CCITG4}$, sorgente binaria con memoria, con $P_X(x) = \{0.96, 0.04\}$. Senza codifica di sorgente, sarebbe necessario un bit per pixel. Usando invece una codifica per sorgente senza memoria, si può scendere a $0.25 ~ \mathrm{bit}/\mathrm{pixel}$, producendo un'entropia $H(X) = 0.247$.
Sfruttando una codifica con memoria monodimensionale (solo sulle righe), si ottiene $0.1 ~ \mathrm{bit}/\mathrm{pixel}$ mentre nel caso bidimensionale si arriva addirittura a $0.05 ~ \mathrm{bit}/\mathrm{pixel}$.

**Esempio 2:** testo italiano. Senza codifica di sorgente, servono $5 ~ \mathrm{bit}/\mathrm{carattere}$. Con codifica di sorgente senza memoria, la codifica ottiene $\ge 3.956 ~ \mathrm{bit}/\mathrm{carattere} = H(X)$. Usando la memoria, si ottiene $< 1 ~ \mathrm{bit}/\mathrm{carattere} = H(X)$. La ridondanza del linguaggio naturale è forte e la memoria è di circa 10 caratteri.
Per eliminare completamente la memoria del linguaggio si dovrebbe costruire un vocabolario in cui ogni sequenza di caratteri sia una parola sensata ed ogni sequenza di parole sia una frase valida. Un linguaggio del genere, però, sarebbe incredibilmente soggetto ad errori di totale incomprensione, in caso di anche un minimo errore o perdita. Insomma, la ridondanza del linguaggio naturale protegge il significato dai disturbi sul canale.

La teoria dell'informazione si occupa di massimizzare l'efficienza della trasmissione su canali disturbati.

# Codifica di canale

> *Definizione:* un canale è un oggetto che racchiude tutti i fenomeni aleatori che fanno sì che il messaggio ricevuto $Y$ a fronte di un trasmesso $X$ sia non perfettamente predicibile per trasmettitore e ricevitore.

```merm
graph LR
X((X)) --> C[Canale] --> Y((Y))
```


I canali possono essere continui o discreti nel tempo. In questo corso sono affrontati solamente canali discreti nel tempo, da cui segue il concetto di *uso di canale* (invio di un singolo simbolo). I canali possono essere anche continui o discreti nelle ampiezze. Dato che spesso i disturbi sono continui, è necessario considerare continui anche i canali che portano segnali ad ampiezza discreta. Infine, anche ai canali si applica la distinzione memoria / non memoria.

Nella pratica si hanno spesso canali con memoria (a causa dei *burst* di disturbi), ma il modello per rappresentarli è complesso. Esistono tecniche di mescolamento dei messaggi per fare in modo che ad ogni uso il canale si comporti in modo indipendente.

Un canale discreto, stazionario senza memoria, con ingresso $X$ e uscita $Y$ è univocamente caratterizzato da:

1. alfabeto $\mathcal X = \{ x_1 ~ x_2 ~ ...~ x_{M_X}\}$ (ingresso discreto) o $\mathcal X = [x_\min, x_\max]$ (ingresso continuo)
2. alfabeto $\mathcal Y = \{y_1 ~ y_2 ~ ... ~ y_{M_Y} \}$ (uscita discreta) o $\mathcal Y = [y_\min, y_max]$ (uscita continua)
3. $P_{Y|X} (y_j|x_i)$ (per ingresso e uscita discreti) o $\mathcal p_{Y|X} (y|x_i)$ (ingresso discreto e uscita continua) o $\mathcal p_{Y|X}$ (ingresso e uscita continui)

Si noti che non è necessario che ingresso e uscita abbiano la stessa cardinalità: può corrispondere un'uscita continua ad un ingresso discreto.


## Distribuzione e densità di probabilità

> $\mathrm{pmf}$: $X$ discreta $\in \mathcal X = \{x_1~...~x_M\}$

$$ P_X(x_i) = \mathrm{Prob}[X = x_i]: \quad 0 \le P_X(x_i) \le 1 ~ \text{ perché } ~ \sum_{i=1}^M P_X(x_i) = 1, ~ \mathbb E[f(x)] = \sum_{i=1}^M P_X(x_i) \cdot P_X(x_i) \cdot f(x_i)$$

> $X$ continua $\in [x_\min, x_\max]$ descritta da una densità di probabilità $p_X(x)$

$$p_X(x_i) \cdot \mathrm d x = \mathrm{Prob}[x \le X \le x + \mathrm d x]$$
$$\mathrm{Prob}[a \le X \le b] = \int_a^b p_X(x) \mathrm d x$$
per cui valgono le proprietà $\int_{-\infty}^\infty p_X(x) = 1$ e $p_X(x) \ge 0$. La funzione $p$ non è limitata a 1, e localmente può assumere valori maggiori di uno a patto che l'integrale complessivamente sia limitato a 1.

Il valore atteso è
$$\mathbb E[p(X)] = \int_{-\infty}^\infty p_X(x) f(x) \mathrm d x.$$


## Tipi di canali

**Canale binario simmetrico BSC($\varepsilon$)**

Canale discreto/discreto, binario/binario.

$$\mathcal X = \{0,1\}, \quad \mathcal Y = \{0, 1\}$$

Il *diagramma di transizione del canale* rappresenta tramite frecce le possibili combinazioni di ingressi e uscite.

```merm
graph LR
Xz(X, 0) -- 1-e --> Yz(Y, 0)
Xu(X, 1) -- 1-e --> Yu(Y, 1)
Xz(X, 0) -- e --> Yu(Y, 1)
Xu(X, 1) -- e --> Yz(Y, 0)
```


**Canale binario con cancellazioni BEC($\varepsilon$)**

```merm
graph LR
X0(X,0) -- 1-e --> Y0(Y, 0)
X0 -- e --> E
X1(X, 1) -- e --> E
X1(X, 1) -- 1-e --> Y1(Y, 1)
```

Le frecce in uscita da un singolo valore sommano a 1, dove $1-\varepsilon = P_{Y|X}(y_j|x_i)$.

**Canale con rumore additivo gaussiano**

Si assume che il rumore $N$ sia indipendente dall'ingresso $X$ e $\sim \mathcal N(0, \sigma^2)$ ($\mathrm{ddp}$ gaussiana di $\mathbb E[\mathcal N] = 0$ e $\mathbb E[\mathcal N^2] = \sigma^2$).

$$p_N(n) = \frac 1 {\sqrt{2 \pi \sigma^2}} e ^{-\frac{n^2}{2 \sigma^2}}$$
I rumori si modellizzano sempre a valor medio nullo, perché la media è la parte predicibile e può essere compensata.

La probabilità che il valore osservato cada tra $-\sigma$ e $\sigma$ è di circa il 65%. L'intervallo $-3\sigma$, $3\sigma$ copre circa il 99.7% dell'area della distribuzione.

GRAFICO:
```
                N
                |
                v
X --> ( canale (+) ) --> Y = X+N
```

$X$ discreto / continuo

$Y$ continua (perché $N$ è continua).

Caso BSC($\epsilon$):

Informazione all'uscita del canale:
$$H(Y) = \sum_{i=1}^{M_y} P_Y(y_i) \log \frac 1 {P_Y(y_i)}$$

Informazione all'ingresso:
$$ H(X) = \sum_{i=1}^{M_X} P_X(x_i) \log \frac 1 {P_X(x_i)}$$

$$\mathrm{Hp}: ~ P_X(x) = \left\{ \frac 1 2, \frac 1 2 \right\} \Rightarrow P_Y(y) = \left\{ \frac{1-\varepsilon} 2 + \frac \varepsilon 2 = \frac 1 2, \frac 1 2 \right\}$$

$$ P_Y(0) = P_X(0) \cdot P_{Y|X} (0|0) + P_X(1) \cdot P_{Y|X}(0|1)$$
(teorema delle probabilità totali)

$$ \Rightarrow H(X) = 1~\mathrm{bit}, \quad H(Y) = 1~\mathrm{bit} $$
se le due variabili sono incorrelate, altrimenti

$$H(X|Y) \le H(X)$$
> *Definizione*: definiamo *informazione mutua* tra $X$ e $Y$
> $$I(x;y) = H(X) - H(X|Y) \quad [\mathrm{bit}/\text{uso di canale}]$$
> e misura l'informazione che $Y$ porta su $X$, cioè l'informazione che attraversa il canale.

*Osservazione 1:* dato che $H(X)$ è sempre maggiore di $H(X|Y)$, allora $I(x;y) \ge 0$, con uguaglianza se e solo se $X$ e $Y$ sono indipendenti. Ovviamente un canale per i quali ingresso e uscita sono completamente incorrelati non porta nessuna informazione.

*Osservazione 2:* $I(x;y) \le H(X)$ perché $H(X|Y) \ge 0$ e $I=H(X)$ se e solo se $X$ e $Y$ sono deterministicamente legati.

> *Definizione*: si definisce *capacità del canale*
> $$ C = \max_{P_X(x)} I(x;y) \quad [\mathrm{bit}/\text{uso di canale}] $$

La capacità è una caratteristica propria del canale.

*Osservazione 3:* $H(X,Y) = H(X) + X(Y|X) = H(Y) + H(X|Y)$ quindi sostituendo
$$ I(x;y) = H(X) - H(X|Y) = \cancel{H(X)} - \cancel{H(X)} + H(Y) - H(Y|X)$$
$$ \Rightarrow I(x;y) = H(Y) - H(Y|X) = I(y;x) $$
e le due formulazioni sono esattamente identiche. Per questo si parla di informazione mutua: è una caratteristica simmetrica.

> *Definizione:* si chiama *equivocazione* $H(X|Y)$ la misura dell'informazione persa nel canale.


Si calcoli ora la mutua informazione per il BSC.

$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$
e la seconda formulazione è generalmente più semplice da calcolare.

$$P_X(x) = \{\frac 1 2, \frac 1 2\} \to H(X) = 1 ~\mathrm{bit}$$
$$P_Y(y) = \{\frac 1 2, \frac 1 2\} \to H(Y) = 1 ~\mathrm{bit}$$

ora:

$$ H(Y|X) = \sum_{i=1}^{M_X} P_X(x_i) \cdot H(Y|x_i)$$
$$ H(Y|x_i) = H(Y|0) = \sum_{j=1}^{M_Y} P_{Y|X}(y_j|0) \cdot \log \frac 1 {P_{Y|X}(y_j|0)} =$$
$$ = (1-\varepsilon) \log \frac 1 {1-\varepsilon} + \varepsilon \log \frac 1 \varepsilon  H_2(\varepsilon) = H(Y|1) $$
quindi
$$ H(Y|X) = \sum_{i=1}^{M_X} P_X(x_i) \cdot H(Y|x_i) = \frac 1 2 H_2(\varepsilon) + \frac 1 2 H_2(\varepsilon) = H_2(\varepsilon) $$

$$ \Rightarrow I(x;y) = 1 - H_2(\varepsilon) $$

![[immagini/Pasted image 20250320164133.png]]
Questo ci dice che un canale che sbaglia sempre ha la stessa utilità di un canale che non sbaglia mai. Un canale che sbaglia sempre sta semplicemente invertendo il segnale - basta saperlo per risolvere il problema. Il canale peggiore è quello che sbaglia nella metà dei casi. In tale scenario, il canale è completamente inservibile.

La generica $P_X(x) ) \{p, 1-p\}$ $\to$ $P_Y(y) = \{ p(1-\varepsilon) + (1-p) \varepsilon, ~ 1 - (p(1-\varepsilon) + (1-p) \varepsilon) \}$.

$$H(Y) = H_2 \left( p(1-\varepsilon)+(1-p)\varepsilon \right)$$
	$$H(Y|X) = pH_2(\varepsilon) + (1-p)H_2(\varepsilon) = H_2(\varepsilon)$$
$$
\begin{cases}
H(Y) = H_2 \left( p(1-\varepsilon)+(1-p)\varepsilon \right) \\
H(Y|X) = pH_2(\varepsilon) + (1-p)H_2(\varepsilon) = H_2(\varepsilon)
\end{cases}
\Rightarrow
I(x;y) = H_2(p(1-\varepsilon) + (1-p) \varepsilon) - H_2(\varepsilon)
$$

$$
C = \left [ \underset{p}{\max} H_2(p(1-\varepsilon) + (1-p)\varepsilon) \right] - H_2(\varepsilon) = 1 - H_2(\varepsilon)
~\Leftrightarrow~
\begin{matrix*}[l]
(1-p) \varepsilon + (1-\varepsilon)p = \frac 1 2 \\
p = \frac{\frac 1 2 - \varepsilon}{1-2\varepsilon} = \frac 1 2
\end{matrix*}
$$

### Tema d'esame 23/4/2009

Canale BEC($\varepsilon$)

```merm
graph LR
X0(X,0) -- 1-e --> Y0(Y, 0)
X0 -- e --> E
X1(X, 1) -- e --> E
X1(X, 1) -- 1-e --> Y1(Y, 1)
```


| $0$ | $\frac{1-\varepsilon} 2$ |     |
| --- | ------------------------ | --- |
| $E$ | $\varepsilon$            |     |
| $1$ | $\frac{1-\varepsilon} 2$ |     |


1. Determinare $I(x;y)$ con $P_X$ uniforme.
2. Con $P_X$ generica calcolare l'equivocazione, quindi $I$ e $C$.



**Punto 1**

$P_X(x) = \{\frac 1 2, \frac 1 2\} \to H(X) = 1$. Si calcola $I(x;y) = H(X)-H(X|Y)$.

$$H(X|Y) = \sum_{i=1}^{M_Y} P_Y(y_i) \cdot H(X|Y_i)$$
- $H(X|Y_j) = \sum_{i=1}^{M_X} P_X(x_i|y_j) \log \frac 1 {P_{X|Y}(x_i|y_j)}$

Il calcolo è breve perché ha solo due termini, ma mancano le $P_X$ condizionate, da ricavare tramite Bayes:

- $P_{X|Y}(x_i|y_j) = P_{Y|X} (y_j|x_i) - \frac{P_X (x_i)}{P_y(y_i)}$

Bisogna calcolarne 6:


|       | $y=0$                                                         | $y=E$                                                   | $y=1$ |
| ----- | ------------------------------------------------------------- | ------------------------------------------------------- | ----- |
| $x=0$ | $(1-\varepsilon)\frac{\frac 1 2}{\frac{1-\varepsilon} 2} = 1$ | $\varepsilon \frac{\frac 1 2}{\varepsilon} = \frac 1 2$ | $0$   |
| $x=1$ | $0$                                                           | $\frac 1 2$                                             | $1$   |
Le sommatorie lungo delle colonne sono a 1.

$$H(X|0) = 1 \log 1 + 0 \log \frac 1 0 = 0 = H(X|1)$$
$$H(X|E) = H_2\left(\frac 1 2\right) = 1$$
$$ \Rightarrow H(X|Y) = \varepsilon \cdot 1 = \varepsilon \quad \text{equivocazione } (P_X \text{ uniforme}) $$
$$ \Rightarrow I(x;y) = 1 - \varepsilon $$

**Punto 2**

$P_X(x) = \{ p, 1-p \}$ generica. $H(X)$ non è più 1 ma $H_2(p)$. $I(x;y)$ resta sempre $1-\varepsilon$.


### Tema 3/11/2006

Un canale con ingresso $X$ ternario ($a,b,c$) e uscita $Y$ quaternaria ($r,s,t,u$) ha il diagramma di transizione riportato in figura (tutte le probabilità di transizione presenti valgono $\frac 1 2$):

```merm
graph LR
a --> r
a --> s
b --> s
b --> t
c --> t
c --> u
```

Calcolare $I(x;y)$ con ingressi equiprobabili.

Ipotizzare una forma di distribuzione ottima di $P_X$ che dipenda da un solo parametro (sfruttando le simmetrie) e calcolare la capacità $C$ del canale. Qual è il codice che raggiunge la capacità? (è sufficiente calcolare la distribuzione $P_X$ ottima...)



**Ingressi equiprobabili**

$P_X = \{\frac 1 3, \frac 1 3, \frac 1 3\}$. $I(x;y) = H(Y)-H(Y|X)$.

| $r$ | $\frac 1 6$ |
| --- | ----------- |
| $s$ | $\frac 1 3$ |
| $t$ | $\frac 1 3$ |
| $u$ | $\frac 1 6$ |

$H(Y) = 2 \frac 1 3 \log 3 + 2 \frac 1 6 \log 6$ $=$ $\frac 1 3 + \log 3 \approx 1.91\bar 3$ bit.

$$H(Y|X) = \sum_{i=1}^3 P_X(x_i) \cdot H(Y|x_i) = H_2\left(\frac 1 2\right) = 1 ~\mathrm{bit} \quad \text{indipendentemente da } P_X$$

$$H(Y|a) = 1$$

$$ \Rightarrow I(x;y) = 0.913 $$



**Distribuzione ottima**

La distribuzione ottima assegna probabilità ai messaggi in base alle probabilità del canale.

| $a$ | $\frac{1-p} 2$ |
| --- | -------------- |
| $b$ | $p$            |
| $c$ | $\frac{1-p} 2$ |
$P_X(x) = \{p, 1-p, p\}$ $\to$ $P_Y(y) = \{\frac p 2, \frac 1 2 - \frac p 2, \frac 1 2 -\frac p 2, \frac p 2\}$.

$$C = \left[\underset p \max H(Y) \right] -H(Y|X) = 2 - 1 = 1 ~ \mathrm{bit}/\text{uso di canale}$$
Questo si ottiene quando tutte le probabilità sono $\frac 1 4$, ovvero per $p = \frac 1 2$.

