
$\mathcal X = \{0,1\}$, $P_X(x) = \{0.2, 0.8\}$ $H(X) = 0.722$ $\mathrm{bit}$

Codifica di Huffman, caso $L=3$:

| $\underline x$ | $P_{\underline x}(\underline x)$ | $\mathcal C ~ \text{(Huffman)}$ | $n_H$ | $n_{GM}$ |
| -------------- | -------------------------------- | ------------------------------- | ----- | -------- |
| 111            | 0.512                            | 1                               | 1     | 1        |
| 110            | 0.128                            | 011                             | 3     | 3        |
| 101            | 0.128                            | 010                             | 3     | 3        |
| 011            | 0.128                            | 001<br>                         | 3     | 3        |
| 100            | 0.0032                           | 00011                           | 5     | 5        |
| 010            | 0.0032<br>                       | 00010                           | 5     | 5        |
| 001            | 0.0032<br>                       | 00001                           | 5     | 5        |
| 000            | 0.008                            | 00000                           | 5     | 7        |

Grafo di Huffman:
```merm
graph LR
A(111, 0.512)
B(110, 0.128)
C(101, 0.128)
D(011, 0.128)
E(100, 0.0032)
F(010, 0.0032)
G(001, 0.0032)
H(000, 0.008)

G --> Z(0.04)
H --> Z

E --> Y(0.064)
F --> Y

Z --> X(0.104)
Y --> X

D --> W(0.232)
X --> W

B --> V(0.256)
C --> V

V --> U(0.488)
W --> U

A --> T(1)
U --> T
```

Lunghezze medie dei messaggi:

- $\bar l_{GM} \approx 2.2$
- $\bar n_{GM} = 0.733$ $\mathrm{bit}$ $\ge$ $0.722$ $\mathrm{bit}$ $= \bar n_H$

Una codifica è tanto buona quanto sono "ben spesi" i bit: ogni bit aggiuntivo spezza la parte rimanente di spazio di probabilità in due. Analizzando la codifica di Huffman per questo esercizio:

- il primo bit separa il caso 111, con probabilità 0.512 $\approx \frac 1 2$ dal resto
- il secondo bit separa il gruppo (110, 101) dal resto (probabilità 0.256 vs 0.232, quasi dimezzamento)

e così via.

> **Esempio**: *Indovina Chi*. 32 personaggi, quindi servono 5 $\mathrm{bit}$ per arrivare alla soluzione. Questo vale se si fanno domande in grado di scartare ogni volta la metà dei personaggi rimasti. Domande che fanno scartare meno della metà dei personaggi portano un $\mathrm{bit}$ di scarsa qualità, e ne serviranno complessivamente più di 5. Al contrario, domande intelligenti che portano a scartare più della metà dei rimasti permettono di abbreviare la partita.


## Ottimalità della codifica di Huffman

*da Cover & Thomas, "Elements of Information Theory", Wiley 1989*

> **Definizione**: $\mathcal C$ è un codice ottimo per la sorgente $X$ se $\nexists$ $\mathcal C': \bar n_{\mathcal C'} < \bar n_{\mathcal C}$, con $\mathcal C$ e $\mathcal C'$ univocamente decodificabili.

> **Teorema**: data $X$ con $M$ messaggi con $\mathcal{pmf}$ $p_1 \ge p_2 \ge p_3 \ge ...\ge p_M$ $\Rightarrow \exists \mathcal C$ immediatamente decodificabile di lunghezze $n_i$, $i = 1...M$, *ottimo*, con le seguenti proprietà:

1. se $p_i \ge p_j$ $\Rightarrow$ $n_i \le n_j$
   
   Altrimenti, scambiando i codici $c(x_i)$ e $c(x_j)$, si otterrebbe $\bar n$ inferiore.
   
2. $n_M = n_{M-1}$: la lunghezza delle ultime due stringhe (messaggi meno probabili) è uguale
   
   Altrimenti, troncando $c(x_M)$ ai suoi primi $n_{M-1}$ $\mathrm{bit}$, si otterrebbe comunque un codice immediatamente decodificabile con $\bar n$ inferiore.
   
3. $c(x_M)$ e $c(x_{M-1})$ possono differire anche solo nell'ultimo $\mathrm{bit}$
   
   Infatti $c(x_M)$ così scelto non inizia come tutti i codici più corti precedentemente assegnati se già $c(x_{M-1})$ lo fa. Se questo codice fosse già assegnato potrei fare uno scambio con un'altra stringa lunga $n_{M-1}$ che deve essere disponibile ($\mathcal C$ immediatamente decodificabile).

### Codifica di Huffman

Data $X$ di messaggi $x_1, ..., x_M$ e probabilità $p_1 \ge p_2 \ge ... \ge p_M$, costruisce $\mathcal C$ ottimo con i seguenti passaggi:

1. Scegliere $c(x_M)$ e $c(x_{M-1})$ di lunghezze $n_M = n_{M-1}$ (incognite) che differiscono solo nell'ultimo bit.
2. Considerare una sorgente $X'$ di messaggi $x'_1, ..., x'_{M-1}$ e probabilità $p'_1 \ge p'_2 \ge ... \ge p'_{M-1}$, con $p'_{M-1} = p_{M-1} + p_M$. Dato un codice $\mathcal C'^{(M-1)}$ ottimo per $X'$, usando $c(x_i) = c(x'_i)$, $i=1,...,M-2$ e $[C(x'_{M-1})~0]$, $[C(x'_{M-1})~1]$ per $x_{M-1}$ e $x_M$ ottengo $\mathcal C$ ottimo per $X$. Infatti:
   $$\bar n_{\mathcal C} = n'_1 p_1 + n'_2p_2 ... (n'_{M-1}+1)(p_M + p_{M-1}) = \bar n_{\mathcal C'} + p_M + p_{M-1}$$
   e se esistesse $\tilde{\mathcal C}^{(M)}$ con $\bar n_{\tilde{\mathcal C}} < \bar n_{\mathcal C}$ con le caratteristiche del teorema, usando $\tilde c(x_1) ... \tilde c(x_{M-2})$ e $\tilde c(x_M)$ senza l'ultimo $\mathrm{bit}$ per $X$ otterrei
   $$\bar n_{\tilde{\mathcal C}^{(M-1)}} = \bar n_{\tilde{\mathcal C}^{(n)}} - p_M - p_{M-1} < \bar n_{\mathcal C'} \Rightarrow \mathcal C' \text{ non è ottimo per } X$$
3. Un codice ottimo per $X'$ lo ottengo procedendo ricorsivamente.

> **Osservazione:** un buon codice di sorgente produce sequenze di $\mathrm{bit}$ con distribuzione uniforme.


**Esempio (1):** standard CCITT4 per la codifica FAX e immagini raster in file PDF e TIFF.

- *sorgente*: immagine bitonale (bianco/nero), descritta a pixel con $P_X(x) = \{ 0.04, 0.96\}$ altamente sbilanciata
	- senza codifica di sorgente: 1 $\mathrm{bit}$ per pixel
	- con codifica di sorgente senza memoria:
	  $$H(x) = 0.247 ~ \mathrm{bit}/\mathrm{pixel}$$
- già con una codifica senza memoria è possibile risparmiare circa il 75% dello spazio; aggiungendo la memoria (i pixel neri sono quasi sempre contigui) si potrebbe risparmiare ancora più spazio

**Esempio (2):** testo italiano.

- $\mathcal X = \{25~\text{ caratteri}\}$
	- senza codifica di sorgente: 5 $\mathrm{bit}/\mathrm{carattere}$
	- con codifica di sorgente senza memoria:
	  $$H(X) = 3.956 ~ \mathrm{bit}/\mathrm{carattere}$$
- le vocali rappresentano circa la metà delle occorrenze pur essendo un quinto delle lettere totali
- altre lingue hanno altri livelli di entropia. Ad esempio l'inglese:
  $$X(x)_{eng} = 4.056 ~ \mathrm{bit}/\mathrm{carattere}$$
  per via della sua maggiore uniformità; la distribuzione dell'italiano è alterata dall'avere una struttura a sillabe che alterna quasi forzatamente vocali e consonanti
- le lingue sono strutture con memoria; si potrebbe ottenere un risparmio di caratteri ancora maggiore usando una codifica con memoria


# Sorgenti con memoria

Se la sorgente ha memoria la conoscenza del messaggio precedente modifica la $\mathrm{pmf}$ del messaggio successivo e quindi l'informazione che questo porta. La distribuzione che fotografa questo legame è la $\mathrm{pmf}$ *condizionata*:
$$P_{X|Y}(x|y) = \mathrm{Prob}[X=x|Y=y]$$
con cui calcoliamo l'*entropia condizionata*.

> **Definizione:** l'*entropia condizionata ad uno specifico evento* $Y=y$ è definita
> $$H(X|y) = \sum_{x \in \mathcal X} P_{X|Y}(x|y) \cdot \log \frac 1 {P_{X|Y}(x|y)}$$
> e misura in $[\mathrm{bit}]$ l'informazione portata da $X$ quando $Y=y$.

**Esempio:** sia $Y$ = carattere precedente = $'q'$.  Allora

$$
P_{X|Y}(x|y) =
\left \{
\begin{matrix}
1 & x = 'u' \\
0 & x \ne 'u'
\end{matrix}
\right .
~\Longrightarrow~
H(X|y) = 0
$$

> **Definizione:** si definisce *entropia condizionata* (media)
> $$H(X|Y) = \sum_{y \in \mathcal Y} P_Y(y) \cdot H(X|y)$$
> e misura l'informazione portata da $X$ quando conosco $Y$.

Per esteso:
$$H(X|Y) = \sum_{y \in \mathcal Y} P_Y(y) \cdot H(X|y) = \sum_{y \in \mathcal Y}\sum_{x \in \mathcal X} \underbrace{P_y(y) \cdot P_{X|Y}(x|y)}_{P_{xy}(x,y) \text{ congiunta}} \log \frac 1 {P_{X|Y}(x|y)} =$$
$$ = \mathbb E_{xy} \left[ \log \frac 1 {P_{X|Y}(x|y)} \right].$$

> **Proprietà:**

1. $X(X|Y) \ge 0$
2. $H(X|Y) \le H(X)$

La proprietà 2 indica che il condizionamento può solo ridurre l'entropia (in media). In particolare $H(X|Y) = H(X) \Leftrightarrow X$ e $Y$ indipendenti.

Si dimostra nel modo seguente, arrivando a stabilire che la differenza tra i due termini sia negativa o nulla:

$$H(X|Y) - H(X) = \sum_x \underbrace{\sum_y P_{xy}(x,y)}_{P_X(x)}~ \log \frac 1 {P_{X|Y}(x|y)} - \sum_x \underbrace{\sum_y P_{xy}(x,y)}_{P_X(x)} \log \frac 1 {P_X(x)} =$$

$$= \sum_y \sum_x P_{xy}(x,y) \log \frac{P_X(x)}{P_{X|Y}(x|y)} \le \log e \left[ \sum_x \sum y P_{xy}(x,y) \left(\frac{P_X(x)P_Y(y)}{P_{xy}(x,y)} - 1\right) \right] = \quad (\log x \le \log e (x-1))$$

$$=\sum_x \sum_y P_X(x)P_Y(y) - \sum_x \sum_y P_{xy}(x,y) = 0.$$


> **Osservazione:** se $X$ e $Y$ sono indipendenti, $H(X|Y) < H(X)$. Il caso estremo è che $X$ e $Y$ siano deterministicamente legate da una funzione $f(\cdot)$ biunivoca tale che $X=f(Y)$:
> $$P_{X|Y} = \left \{ \begin{matrix}1 & x=f(y) \\0 & x \ne f(y)\end{matrix} \right. ~\Rightarrow~ H(X|Y) = 0.$$
> Non è vero che $H(X|y) \le H(X)$. In particolare se $P_{X|Y}(x|y)$ è un po' "più uniforme" di $P_X(x)$ può capitare che $H(X|y) > H(x)$.

Analogamente possiamo definire:

> **Definizione:** si dice *entropia congiunta tra due variabili casuali* il valore
> $$H(X,Y) = \mathbb E_{xy} \left[ \log \frac 1 {P_{xy}(x,y)} \right] = \sum_x \sum_y P_{xy}(x,y) \cdot \log \frac 1 {P_{xy}(x,y)}$$
> che misura l'informazione in $[\mathrm{bit}]$ portata congiuntamente da $X$ e $Y$.

> **Osservazione:** poiché $P_{xy}(x,y) = P_{X|Y}(x|y) \cdot P_Y(y)$ nel logaritmo della formula dell'entropia congiunta, risulta:
> $$H(X,Y) = H(X|Y) + H(Y).$$
> Se $X$ e $Y$ sono indipendenti, $H(X|Y)$ diventa $H(X)$ e dunque la congiunta si riduce alla somma delle entropie dei singoli.
> 
> Cambiando formulazione:
> $$H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X).$$

Sia l'entropia congiunta che l'entropia condizionata sono applicabili a più variabili casuali, in numero maggiore di 2. È possibile condizionare una variabile a più altre variabili:

$$H(X|Y,Z) = \mathbb E_{xyz} \left[ \log \frac 1 {P_{X|YZ}(x|y,z)} \right]$$
che misura in $[\mathrm{bit}]$ l'informazione che porta $X$ quando sono noti sia $Y$ che $Z$. Si osservi che

$$H(X|Y,Z) \le H(X|Y) \le H(X),$$
$$H(X|Y,Z) \le H(X|Z) \le H(X),$$
ovvero ogni condizionamento può solo ridurre l'entropia.

Si ricordi che

$$ P_{X|YZ} (x|y,z) = \frac{P_{XYZ}(x,y,z)}{P_{YZ}(y,z)}. $$

Applicando lo stesso all'entropia congiunta:

$$H(x,y,z) = \mathbb E_{XYZ} \left[ \log \frac 1 {P_{XYZ}(x,y,z)} \right]$$
da cui

$$H(X,Y,Z) = H(Z) + H(Y|Z) + H(X|Y,Z)$$

perché $P_{XYZ}(x,y,z) = P_{X|YZ}(x|y,z) \cdot P_{YZ}(y,z)$ = $P_{X|YZ}(x|y,z) \cdot P_{Y|Z}(y|z) P_Z(z)$.


## Entropia di sorgenti con memoria

> **Definizione:** *informazione media per messaggio emessa da una sorgente con memoria* $X$:
> $$H(X) \triangleq \lim_{L \to \infty} H(X_k|X_{k-1}~...~X_{k-L})$$

Se la sorgente ha memoria finita $m$ (*sorgente di Markov*) allora:

$$P_{X_k|X_{k-1},...,X_{k-m-1}}(x_k|x_{k-1},...,x_{k-m-1}) = P_{X_k|x_{k-1},...,x_{k-m}}(x_k|x_{k-1},...,x_{k-m})$$

$$ \Rightarrow H(X) = H(X_k|X_{k-1}, ..., X_{k-m})$$

In alternativa:

$$H(X) = \lim_{L \to \infty} \frac 1 L H(X_k, x_{k-1}, ..., X_{k-L+1}).$$

Le due definizioni coincidono senza altre ipotesi con memoria finita $m$.


> **Estensione del teorema** *sulla codifica di sorgente*: sia $X$ con memoria e $\mathcal C$ un codice univocamente decodificabile per $\underline X = (x_k, ..., x_{k-L+1})$. Allora
> $$ \Rightarrow \quad \bar n_{\mathcal C} \ge \frac 1 L H(\underline X) $$

**Dimostrazione:** se $\mathcal C$ è univocamente decodificabile allora le sue lunghezze $l(\underline x)$ soddisfano la disuguaglianza di Kraft: $\sum_{\underline x} 2^{-l(\underline x)} \le 1$. Allora

$$ \frac 1 L H(\underline X) - \bar n = \frac 1 L \left( H(\underline X) - \bar l \right) ... \underset{\underset{\text{ Kraft}}{\uparrow}}{\le} 0. $$


---

### Esercizi


**Esercizio:** sia data una sorgente binaria $\mathcal X = \{0, 1 \}$, con $P_X(x) = \{\frac 1 2, \frac 1 2\}$ con $\mathrm{pmf}$ condizionata data da:

1. $P_{X_k | X_{k-1}, X_{k-2}} (x_k| 0~1) = P_{X_k | X_{k-1}, X_{k-2}} (x_k| 1~0)  = \frac 1 2$, $x_k = 0,1$.
   
   A parole: se gli ultimi due bit sono diversi tra loro, il bit corrente ha distribuzione uniforme
   
2. $P_{X_k | X_{k-1}, X_{k-2}} (1| 1~1)$ = $P_{X_k | X_{k-1}, X_{k-2}} (0| 0~0)  = p$, da cui $P_{X_k | X_{k-1}, X_{k-2}} (0| 1~1)$ = $P_{X_k | X_{k-1}, X_{k-2}} (1| 0~0) = 1-p$

Calcolare $H(X)$ e discutere in particolare i casi $p=0$, $p=\frac 1 2$, $p=1$ (caso limite).


 > **Osservazione:** la sorgente ha memoria $m=2$ (sorgente di Markov).

$$H(X) = X(X_k | X_{k-1}, X_{k-2}) = \mathbb E_{X_k X_{k-1} X_{k-2}} \left[ \log \frac 1 {P_{X_k | X_{k-1}, X_{k-2}}(x_k|x_{k-1}, x_{k-2})} \right]$$

$$ = \sum_{x_k=0}^1 ~ \sum_{x_{k-1}=0}^1 ~ \sum_{x_{k-2}=0}^1 P_{X_k|X_{k-1}X_{k-2}}(x_k|x_{k-1},x_{k-1}) \cdot P_{X_{k-1}X_{k-2}}(x_{k-1}, x_{k-2}) \log \frac 1 {P_{X_k|X_{k-1}X_{k-2}}(x_k|x_{k-1},x_{k-2})}. $$

Sono note le $P_{X_k|X_{k-1}X_{k-2}}(x_k|x_{k-1},x_{k-1})$. Non sono invece note le *probabilità di stato* $P_{X_{k-1}X_{k-2}}(x_{k-1}, x_{k-2})$.

Esse si ricavano tramite una *catena di Markov*:

```merm
graph LR

S((stato x_k-1, x_k-2)) --> A((00))

A -- p --> A
A -- 1-p --> B((10))
B -- 1/2 --> C((01))
C -- 1/2 --> B
C -- 1/2 --> A
B -- 1/2 --> D((11))
D -- 1-p --> C
D -- p --> D


```


$$
\begin{cases}
P_{\underline X} (00) = p P_{\underline X}(00) + \frac 1 2 P_{\underline X}(01) \\
P_{\underline X} (10) = \frac 1 2 P_{\underline X}(01) + (1-p) P_{\underline X}(00) \\
P_{\underline X} (01) = (1-p) P_{\underline X}(11) + \frac 1 2 P_{\underline X}(10) \\
P_{\underline X} (00) + P_{\underline X}(11) + P_{\underline X}(01) + P_{\underline X}(10) = 1
\end{cases}
$$


Sappiamo che: 

- $P_u = P_{\underline X} (0,0) = P_{\underline X}(1,1)$
- $P_d = P_{\underline X}(0,1) = P_{\underline X}(1,0)

quindi possiamo ridurre il sistema a due incognite e dunque a due equazioni:

$$
\begin{cases}
P_u = p P_u + \frac 1 2 P_d \\
2 P_u + 2 P_d = 1
\end{cases}
~
\begin{cases}
P_u - p P_u - \frac 1 4 + \frac 1 2 P_u = 0 \\
P_d = \frac 1 2 - P_u
\end{cases}
~
\begin{cases}
P_u = \frac{\frac 1 2}{3-2p}= P_{\underline X}(00) = P_{\underline X}(11) \\
P_d = \frac{1-p}{3-2p}= P_{\underline X}(01) = P_{\underline X}(10)
\end{cases}
$$


Ora:

$$H(X) = 2 P_u \cdot \underbrace{\left( p \log \frac 1 p + (1-p) \log \frac 1 {1-p} \right)}_{H_2(p)} + 2 P_d = $$

$$ = \frac{H_2(p)}{3-2p} + \frac{2(1-p)}{3-2p} = \frac{H_2(p) + 2 - 2p}{3-2p} $$

Analisi caso per caso:

- $p=0$ $\to$ $H(X) = \frac 2 3$
	- 1 $\mathrm{bit}$ con probabilità $\frac 2 3$
	- 0 $\mathrm{bit}$ con probabilità $\frac 1 3$

```merm
graph LR
A((00, p = 1/6))

A -- 1 --> B((10, p = 1/3))
B -- 1/2 --> C((01, p = 1/3))
C -- 1/2 --> B
B -- 1/2 --> D((11, p = 1/6))
C -- 1/2 --> A
D -- 1 --> D
D -- 1 --> C
```

- $p = \frac 1 2$ $\to$ $H(X) =1$

```merm
graph LR
A((00))

A -- 1/2 --> B((10))
B -- 1/2 --> C((01))
C -- 1/2 --> B
B -- 1/2 --> D((11))
C -- 1/2 --> A
D -- 1/2 --> D
D -- 1/2 --> C
```

- $p=1$ $\to$ $H(x) = 0$

```merm
graph LR
A((00)) -- 1 --> A
B((10)) -- 1/2 --> C((01))
C -- 1/2 --> B
B -- 1/2 --> D((11))
C -- 1/2 --> A
D -- 1 --> D
```



con $H_2(p)$ = ![[hx.png]]

---

**Tema d'esame 16/7/2009**

Un anemometro rileva l'assenza di vento (0) o la sua direzione (+1,-1) campionando la situazione a intervalli di tempo regolari. L'analisi statistica dei dati misurati rivela che se in un dato istante non c'è vento, nel successivo rimane calmo con probabilità 1/2, o si alza vento con probabilità 1/4 per ciascuna delle due direzioni. Se invece si è già in presenza di vento in. una qualsiasi direzione, all'istante successivo questo si calma oppure si mantiene uguale con probabilità 1/4 o 3/4, rispettivamente.

Sia $X$ la sorgente ternaria di alfabeto (-1,0,+1) che modellizza nel modo descritto i messaggi rilevati dall'anemometro. Si tratta di una sorgente con memoria?

...

```merm
graph LR
A((0)) -- 1/2 --> A
A -- 1/4 --> B((+1))
A -- 1/4 --> C((`-1))

B -- 3/4 --> B
C -- 3/4 --> C
B -- 1/4 --> A
C -- 1/4 --> A
```

**Altri temi d'esame:**

- 16/7/2009
- 24/10/2013
- 26/6/2014
- 17/1/2019
