
# Validazione, cross-validazione e regolarizzazione

Si può interpretare l'errore *out-of-sample* come la somma dell'errore *in-sample* con la penalità dovuta alla complessità. L'errore *out-of-sample* complessivo si stima per validazione, mentre la penalità si stima per regolarizzazione. Queste due tecniche non sono utilizzate solo per i modelli di machine learning ma anche per l'identificazione dei modelli dinamici Questi ultimi sono modelli di regressione ad apprendimento supervisionato per segnali a tempo continuo.

L'obiettivo dell'uso dell'insieme di validazione è la stima delle prestazioni *out-of-sample* del modello. Si rimuove un sottoinsieme di dati dal totale, si stima il modello sui dati rimanenti e infine se ne verificano le prestazioni sui dati rimossi. Solitamente nella divisione si tende ad assegnare il 70% dei dati al sottoinsieme di addestramento e il restante 30% alla validazione. Tale proporzione è da aggiustare in modo più fine sulla base delle curve di apprendimento. Utilizzando un grande sottoinsieme di validazione si ottiene una migliore stima dell'errore, ma essa sarà riferita ad un modello mal stimato, perché allargare l'insieme di validazione significa restringere quello di addestramento. Dopo la fase di validazione, se il modello è accettabile, si procede a riaddestrarlo sulla totalità dei dati. Ci si aspetta che il modello addestrato sui dati interi abbia mediamente un errore minore o uguale rispetto all'errore stimato dalla validazione.
La divisione dei dati si effettua di solito con una tecnica chiamata *stratified sampling*. Essa mantiene il medesimo bilanciamento delle classi in entrambi i sottoinsiemi di addestramento e di validazione. Lo sbilanciamento di classi, seppure sempre da evitare, è meno problematico nei problemi di regressione che in quelli di classificazione. Esiste una teoria, chiamata teoria PAC, che fornisce dei limiti superiori teorici all'errore *out-of-sample* , che tuttavia spesso risultano talmente elevati da essere inutili.

La validazione è utilizzata per risolvere due diverse categorie di problemi. La prima, già affrontata, riguarda la valutazione delle prestazioni del modello. La seconda, invece, riguarda la scelta del modello migliore tra un insieme di modelli possibili. Tra queste decisioni ricadono la scelta del tipo di modello (lineare, non lineare, ...), la scelta del numero di regressori, la scelta del parametro di regolarizzazione $\lambda_{reg}$, e varie altre scelte riguardanti l'architettura del modello e i suoi iperparametri. Per questo secondo tipo di validazione si addestrano e validano i diversi modelli sulla stessa suddivisione di dati tra sottoinsiemi di addestramento e di validazione, per poi scegliere il modello con l'errore di validazione minore. Per la scelta di iperparametri, si testano varie versioni dello stesso modello con valori dell'iperparametro scelti da una griglia tra un valore minimo e un valore massimo.

L'uso ripetuto dello stesso insieme di validazione può causare una nuova forma di *overfitting*. Scegliere iperparametri in base allo stesso sottoinsieme di validazione sul quale si prova il modello, infatti, ottimizza le prestazioni di quest'ultimo su quegli specifici dati. Per questo motivo è utile fare una ulteriore divisione nei dati. Si separa un'ulteriore porzione di dati, chiamata *sottoinsieme di test*, sul quale valutare il modello finale. Questo insieme non è riutilizzabile, pena incorrere nello stesso errore che si è cercato di evitare separando la fase di validazione dalla fase di prova. Il sottoinsieme di addestramento è totalmente contaminato, quello di validazione lo è parzialmente; l'unico completamente incorrelato al modello è il sottoinsieme di prova. Utilizzare i dati di addestramento o di validazione per la prova causa una stima al ribasso dell'errore *out-of-sample*. Le più comuni proporzioni nei quali dividere i dati sono: 60% addestramento, 20% validazione, 20% prova.

Non sempre il numero di dati è sufficiente per una divisione in tre sottoinsiemi. Questa insufficienza si riconosce osservando le curve di apprendimento: se l'errore *in-sample* risulta eccessivamente elevato in corrispondenza del 70-80% dei dati disponibili, la divisione non è accettabile. Utilizzando questi dati per l'addestramento, ne risulterebbe un modello insufficiente. Aumentare la proporzione ridurrebbe l'efficacia del sottoinsieme di validazione. Per risolvere questa problematica si introduce una tecnica chiamata *validazione incrociata*. Il tipo più semplice è chiamato *leave-one-out*. In questa variante si addestra il modello su $N-1$ dati e lo si valida sul singolo dato restante. Questo crea $N$ possibili coppie insieme di addestramento / dato di validazione. L'errore di validazione complessivo, detto *errore di cross-validazione*, è la media degli $N$ errori di validazione calcolati per le coppie addestramento / dato di validazione. Una metrica utile è la deviazione standard campionaria dei singoli errori di validazione. Valori elevati di tale varianza indicano grande sensibilità ai dati di addestramento, che a sua volta è sintomo di *overfitting*. Una tecnica alternativa è la validazione incrociata a $k$ pieghe (*k-fold*), che divide i dati in $k$ porzioni, di cui $k-1$ da usare per l'addestramento, e una per la validazione. Si calcola poi l'errore di cross-validazione come media dei $k$ errori di validazione ottenuti da tutte le possibili combinazioni di dati di addestramento e di validazione. Questa variante, rispetto alla *leave-one-out*, implica una minore complessità computazionale e produce una stima dell'errore *out-of-step* con varianza inferiore. Indicativamente un buon compromesso è scegliere $k=10$.

Come già descritto, una sproporzione di regressori rispetto al numero di dati di addestramento tende a causare *overfitting*. La complessità ottimale del modello non è tuttavia una semplice funzione del numero di dati, ma anche di altri fattori legati alla loro qualità. Per questo può risultare utile effettuare una selezione dei regressori. Si scelgono dunque i regressori con la massima correlazione univariata all'uscita, per validazione incrociata; si addestra il modello su tali regressori; si utilizza nuovamente la validazione incrociata per stimare l'errore *out-of-sample* ed eventuali iperparametri. Ogni scelta legata alla configurazione deve essere sottoposta a validazione incrociata.

Le *formule di complessità ottima* sono strumenti matematici per casi ancora più estremi rispetto a quelli che richiedano validazione incrociata. Esse permettono di stimare la complessità ottimale del modello assumendo di utilizzare per il solo addestramento la totalità dei dati, dunque senza lasciare sottoinsiemi ai fini della validazione o della prova. Sono state sviluppate in tempi in cui la raccolta, lo stoccaggio e la suddivisione dei dati erano lente e laboriose, impedendo di averne abbastanza per poter effettuare la validazione secondo gli approcci precedentemente descritto. Al contrario di metodi come la regolarizzazione, le formule di complessità ottima regolano la complessità in modo discreto. Una delle formule di complessità ottima è nota come *Akaike Information Criterion*, oppure come *Final Prediction Error* quando impiegata in ambito di sistemi dinamici:
$$AIC(d) = 2 \cdot \frac d N + \ln[J(\hat \theta_N; d)].$$
Questa formula tende a sovrastimare l'ordine con probabilità non nulla quando il campione contiene più di otto dati e il modello appartiene alla stessa classe della funzione origine dei dati (ad esempio quando si usa un modello lineare su dati distribuiti linearmente). Esiste dunque un criterio alternativo, chiamato *Minimum Description Length*, che è asintoticamente corretto:
$$MDL(d) = \ln[N] \cdot \frac d N + \ln[J(\hat \theta_N; d)]$$
L'assunzione che rende impreciso il criterio di Akaike è raramente verificata, quindi nella maggior parte dei casi d'uso si preferisce quest'ultimo rispetto all'MDL per la maggiore velocità di calcolo. In entrambe le formule il primo termine è crescente all'aumentare della complessità del modello, e il secondo termine è decrescente. È dunque possibile trovare un $d$ (discreto) che sia argomento minimo, e che rappresenti l'ordine del modello a complessità ottima.

Per ricapitolare: dove possibile, è ideale utilizzare la suddivisione in sottoinsiemi di addestramento, validazione e prova; quando non è possibile, si impiega la validazione incrociata; in caso di estrema penuria di dati si ricorre alle formule di complessità ottima. Si anticipa che, nei modelli dinamici, queste tecniche rimangono applicabili ma è fondamentale ricordare che i dati sono una serie storica, e dunque il loro ordine non va rotto nel suddividere l'insieme.

# Stima bayesiana
Si introduca ora la *stima bayesiana*. Essa si basa sull'assunzione che i parametri, come i dati, siano variabili casuali. A questo fine, recuperiamo alcuni concetti di statistica multivariata. Supponiamo di avere due variabili casuali discrete e binarie $a$ e $b$. Definiamo *distribuzione di probabilità congiunta* la tabella delle probabilità $P(a,b)$ della realizzazione di coppie di $a$ e $b$. La sommatoria di tutti i valori della tabella delle probabilità congiunte deve essere 1. $$\sum_{a=0}^1 \sum_{b=0}^1 p(a,b) = 1$$

|       | $b=0$ | $b=1$ |
| ----- | ----- | ----- |
| $a=0$ | $0.4$ | $0.3$ |
| $a=1$ | $0.2$ | $0.1$ |

Si definisce *distribuzione marginale* la distribuzione congiunta di un sottoinsieme di variabili casuali. Nel caso discreto il calcolo si riconduce alla somma delle probabilità di interesse lungo l'asse delle variabili non di interesse, all'interno della matrice delle probabilità congiunte. Nel caso continuo la somma è sostituita da un'integrazione. Con una coppia di variabili casuali binarie, l'operazione equivale alla sommatoria di un'intera riga o colonna della tabella delle congiunte.

|               | $b=0$        | $b=1$        | **congiunte**  |
| ------------- | ------------ | ------------ | -------------- |
| $a=0$         | $0.4$        | $0.3$        | $P(a=0) = 0.7$ |
| $a=1$         | $0.2$        | $0.1$        | $P(a=1) = 0.3$ |
| **congiunte** | $P(b=0)=0.6$ | $P(b=1)=0.4$ |                |

Definiamo *distribuzione condizionata* la distribuzione delle probabilità al restringersi della popolazione considerata. Al realizzarsi di uno specifico valore di una variabile casuale, ridistribuiamo la probabilità delle possibili realizzazioni delle variabili casuali restanti in modo che sommino nuovamente a 1. Dati $N_A$ e $N_B$, $P(A,B) = \frac {N_{AB}} N$ e dunque
$$ P(A|B) = \frac{N_AB/N}{N_B / N} = \frac{P(A,B)}{P(B)}.$$
Si introduca ora il *teorema di Bayes*. Dato che $P(A,B) = P(B,A)$ e che $P(B,A) = P(B|A)P(A)$, allora $P(A|B)P(B) = P(B|A) P(A)$, e quindi :
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}.$$
Esso permette di ridistribuire la probabilità di $A$ a posteriori dell'informazione portata da $B$. Notare che "a posteriori" non intende sequenzialità temporale, ma solo di conoscenza. $P(B)$ agisce come una sorta di fattore di normalizzazione. La probabilità condizionata degenera al prodotto tra le probabilità delle due classi soltanto quando i due eventi sono tra loro indipendenti (ovvero quando $P(A|B) = P(A)$ e $P(B|A) = P(B)$.

Il principio della probabilità a posteriori introdotto dal teorema di Bayes può essere applicato alla stima dei parametri dei modelli. Invece che vedere il vettore dei parametri $\theta$ come una variabile vettoriale deterministica, possiamo interpretarlo come una variabile casuale dotata di una certa distribuzione. Tale distribuzione è scelta in base alle conoscenze a priori del creatore del modello. Informazioni relative alla natura del fenomeno osservato possono, ad esempio, portare ad aspettarsi che il parametro abbia un certo valore atteso, e che le sue possibili realizzazioni si ammassino attorno a tale valore atteso secondo una certa distribuzione. Questa distribuzione è a priori, perché costruita prima della raccolta dei dati. I dati effettivi rappresentano invece l'informazione a posteriori in base alla quale aggiustare la distribuzione. L'informazione, nella stima bayesiana, è dunque portata da due elementi: la distribuzione a priori, e l'informazione dei dati. Quest'ultima è interpretabile come la verosimiglianza.
$$\underset{\text{a posteriori}}{f_{\theta|Y} (\theta|Y)} = \frac{ \overset{\text{verosimiglianza}}{f_{Y|\theta}} \cdot \overset{\text{a priori}}{f_\theta (\theta)}}{\underset{\text{verosimiglianza marginale}}{f_Y(Y)}}$$Possiamo insomma aggiustare la verosimiglianza a posteriori in base a quello che sappiamo a priori sulla distribuzione dei dati, prima di raccoglierli effettivamente. La stima bayesiana è un compromesso tra la stima a priori e la stima di verosimiglianza proveniente dai dati. Si tratta di una sorta di regolarizzazione. La stima che massimizza la verosimiglianza a posteriori è detta *MAP (Maximum A Posteriori)*, definita come $\hat \theta = \arg \max_\theta f_{\theta | Y}(\theta | Y)$. Tecniche alternative per ricavare valori puntali dalla distribuzione a posteriori includono anche il *valore atteso a posteriori* ($\hat \theta = \mathbb E_\theta [f_{\theta | Y}(\theta | Y)] \equiv \mathbb E[\theta|Y]$) e la mediana.

Il calcolo della distribuzione a posteriori è solitamente complesso per via analitica. Per questo, spesso si utilizza una tecnica di integrazione numerica nota come *Markov Chain Monte Carlo (MCMC)*. Il calcolo è invece immediato quando sia la distribuzione a priori che la distribuzione dei dati sono gaussiane. In tal caso, anche la distribuzione a posteriori è gaussiana.

