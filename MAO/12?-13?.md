# Problema del commesso viaggiatore

Un *ciclo hamiltoniano* passa una e una sola volta per tutti i nodi di un grafo tornando infine al nodo iniziale. Il *problema del commesso viaggiatore* (*TSP, Travelling Salesman Problem*) consiste nel trovare il ciclo hamiltoniano di costo minimo tra tutti i possibili in un grafo. Il costo è definito come somma dei pesi degli archi lungo il ciclo.

Si formuli il problema di *TSP simmetrico*. La funzione obiettivo consiste nella somma dei pesi degli archi lungo il ciclo hamiltoniano. Vigono *vincoli di assegnamento*: ogni nodo deve avere esattamente un arco entrante e un arco uscente. Devono inoltre essere imposti vincoli per evitare la formazione di sottocicli: scelto un sottoinsieme qualsiasi di nodi, deve esistere almeno un arco uscente da uno di essi che porti ad un nodo non appartenente a tale sottoinsieme. In una formulazione alternativa, il numero di vincoli di connessione deve essere pari al numero di tagli tra il sottogruppo e i nodi restanti.

Il TSP può essere trattato come un *problema di ottimizzazione combinatoria*. L'insieme delle soluzioni ammissibili è quello di tutti i cicli hamiltoniani possibili sul grafo considerato. Se il grafo è orientato e completo, il numero di cicli hamiltoniani possibili è pari al numero di nodi, meno uno. Se il grafo non è orientato, il numero di cicli hamiltoniani è la metà rispetto al caso precedente. Risolvere il problema procedendo per enumerazione completa è possibile soltanto se il grafo è di piccole dimensioni: il numero di iterazioni richieste è infatti pari al fattoriale del numero di nodi. Fortunatamente esistono metodi alternativi basati su algoritmi *greedy*.

## Algoritmo *greedy* per TSP

L'algoritmo si inizializza scegliendo il nodo iniziale come nodo corrente, e lasciando vuoto l'insieme dei nodi nel ciclo hamiltoniano da trovare. Ad ogni iterazione si individuano tutti gli archi tra il nodo corrente e i nodi non ancora inseriti nell'insieme dei nodi del ciclo. Si seleziona tra tali archi il più promettente secondo un'euristica e lo si aggiunge all'insieme. Si sceglie infine il nodo destinazione dell'arco scelto come nodo da considerare per l'iterazione successiva.

Il cirterio più comune è noto come *nearest neighbor* e consiste nello scegliere sempre l'arco di costo minore. Se il grafo è completo l'algoritmo riuscirà sicuramente a trovare un ciclo hamiltoniano. Se non lo è, può capitare che, a una data iterazione, tutti gli archi uscenti dal nodo considerato portino a nodi già inseriti nell'insieme. L'algoritmo si blocca e non viene trovato un cammino hamiltoniano anche se esso esiste. L'algoritmo è veloce ma può prendere strade non ottimali che non è in grado di correggere, perché le scelte passate non convenienti non vengono abbandonate.

Per migliorare una soluzione già trovata, esiste una procedura chiamata *local search* che applica il *2-scambio* al ciclo hamiltoniano trovato. Il 2-scambio consiste nello scambiare le destinazioni tra due coppie di nodi adiacenti. Ad esempio, avendo la connessione `1-2-3-4`, composta dalle coppie `1-2`e `3-4`, il 2-scambio la trasforma in `1-4-3-2`. Le possibilità di miglioramento del costo del cammino aumentano al crescere delle dimensioni del grafo, ma aumenta anche il costo elaborativo della procedura.

## TSP metrico

La tecnica del *TSP metrico* è basata sulla disuguaglianza triangolare. L'algorimto sceglie un arco soltanto se non esiste un percorso alternativo di costo inferiore passante per un terzo nodo intermedio. L'euristica *twice around* ha un errore relativo massimo pari a 1. Si costruisce un albero di copertura a costo minimo a partire dal nodo iniziale. Si duplicano i suoi archi in modo da avere coppie di archi direzionati, entrambi di costo pari all'arco non orientato originale, che vadano verso e tornino da ogni nodo. Seguendo gli archi si trova un cammino che copre tutti i nodi in modo ordinato. Da questo grafo è possibile determinare un cammino hamiltoniano. Dove è conveniente, si seguono direttamente gli archi. In altri casi, è meno costoso passare direttamente da un nodo al successivo tramite un collegamento diretto che faccia da scorciatoia. Il cammino hamiltoniano risultante ha un costo compreso tra quello del cammino hamiltoniano a costo minimo e quello del cammino trovato lungo il grafo con gli archi a due vie considerato senza le scorciatoie. Il costo del secondo è pari al doppio del costo dell'albero di copertura determinato all'inizio, dato che è stato costruito raddoppiandone gli archi. L'errore relativo.

## Branch-and-bound per TSP simmetrico

Il TSP simmetrico può essere risolto usando un metodo *branch-and-bound*. Per fare questo, è necessario interpretare il ciclo hamiltoniano come un *cammino hamiltoniano* unito ad un lato di chiusura. Il cammino hamiltoniano non è altro che un albero di supporto a costo minimo con nodi di grado minore o uguale a due.
Unendo un albero di supporto a costo minimo al lato di costo minimo tra quelli non appartenenti all'albero, otteniamo un 1-albero il cui costo è limite inferiore per il valore ottimo del TSP. Si tratta di un limite inferiore e non di una soluzione perché il lato di costo minimo non corrisponde necessariamente al lato che serve per chiudere l'albero trasformandolo in un ciclo hamiltoniano. La soluzione di questo problema è dunque un rilassamento del TSP che può produrre o non produrre una soluzione ammissibile per il problema originale.

Se l'albero ottenuto non genera un ciclo hamiltoniano, si effettua il *branching*: si sceglie il nodo di grado massimo e si cerca tra i suoi lati incidenti quello di costo minimo. Se il risultato è un ciclo hamiltoniano, si chiude il ramo e si annota la soluzione ammissibile trovata. Se non lo è, si chiude il ramo. Il limite inferiore per il costo è determinato scegliendo, tra le soluzioni non ammissibili, quella a costo minimo. Su ogni ramo, non è possibile utilizzare come lato di chiusura un arco che sia già stato utilizzato in una iterazione precedente su quello stesso ramo; in tal caso si sceglie il secondo migliore candidato, o il terzo, e così via. I lati già utilizzati sono detti *lati dominati*. Una volta conclusa la procedura di *branching*, si seleziona la soluzione ammissibile di costo minimo tra quelle trovate.

# Minimal Cardinality Machine Scheduling

Un certo numero di lavori, con uno specifico momento d'inizio e una durata definita, deve essere eseguito su un certo numero di macchine, tutte uguali tra loro. L'obiettivo è utilizzare il minor numero di macchine evitando allo stesso tempo sovrapposizioni tra i lavori.

Il problema è rappresentabile come un PLI. Serve un numero di variabili binarie pari al prodotto del numero di macchine per il numero di lavori. Queste variabili rappresentano gli assegnamenti dei lavori alle macchine. Per assicurarsi che ogni lavoro sia assegnato esattamente ad una e una sola macchina, si stabiliscono dei *vincoli di semiassegnamento*. Due lavori possono essere eseguiti sulla stessa macchina solo se i loro periodi di esecuzione non si sovrappongono. I *vincoli di non sovrapposizione* si ricavano da una matrice di compatibilità tra lavori. Il numero di tali vincoli dipende dal numero di conflitti e ha un limite superiore dipendente dal numero di lavori e di macchine. La funzione obiettivo, infine, è il numero di macchine utilizzate.

Il problema può essere risolto con algoritmi *greedy*. Al momento dell'inizializzazione nessun lavoro è assegnato e tutte le macchine sono scariche. Ad ogni iterazione si seleziona un lavoro secondo un'euristica. Secondo un'altra euristica, si decide a quale macchina assegnare il lavoro considerato, dando priorità alle macchine già in uso. Solitamente l'euristica per la scelta dei lavori consiste nel procedere in ordine di istante di inizio, mentre le macchine vengono considerate secondo un ordine arbitrario ma fissato all'inizio della risoluzione. Questa versione dell'algoritmo porta alla soluzione ottima.

# Minimal Makespan Machine Scheduling

Il problema è simile al precedente. In questo caso non è dato un istante di inizio prefissato per i singoli lavori. L'obiettivo è minimizzare il tempo complessivo di esecuzione. Questo tempo corrisponde al tempo di lavoro della macchina più carica. È possibile interpretarlo come un problema di partizione. Le variabili decisionali sono binarie e il loro numero è pari al prodotto del numero di lavori e di macchine. Anche in questo problema vigono vincoli di semiassegnamento.

Per poter rendere il problema in una formulazione riconducibile alla PLI, è necessario definire un maggiorante dei tempi di esecuzione,e prenderlo come funzione obiettivo da minimizzare. Sarebbe altrimenti necessario cambiare funzione obiettivo ogni volta che, nel corso della risoluzione, cambia la macchina più carica in seguito a nuovi assegnamenti. Nella soluzione ottima, il maggiorante diventa esattamente pari al tempo di esecuzione della macchina più carica.

Anche per questo problema esiste una soluzione mediante algoritmo *greedy*. All'inizializzazione tutte le macchine sono scariche e nessun lavoro è assegnato. I lavori vengono ordinati per durata, in ordine non decrescente (SPT) o non crescente (LPT). Ad ogni iterazione il primo lavoro nella lista viene assegnato alla macchina più scarica, e dopodiché si aumenta il tempo di lavoro della macchina sommandovi il tempo di esecuzione del lavoro assegnato. Generalmente l'ordinamento LPT produce soluzioni di qualità migliore.

È possibile valutare l'errore massimo relativo. Esso corrisponde alla differenza tra il tempo della soluzione ideale e il tempo della soluzione reale, divisa per quest'ultima. Le soluzioni con tempo minore di quella ideale sono ovviamente non ammissibili. La soluzione ideale si ottiene rilassando l'integralità dei lavori e dividendo uniformemente il tempo totale di lavoro sul numero di macchine. Si tratta di un limite inferiore e non di una vera soluzione. La stima del caso peggiore, invece, si ottiene distribuendo uniformemente il tempo di tutti i lavori tranne l'ultimo sulle macchine, e poi assegnando l'ultimo lavoro ad una macchina qualsiasi. La stima dell'errore massimo relativo è pari alla differenza tra il tempo massimo e il tempo minimo, diviso il tempo minimo. L'errore massimo relativo è anche riconducibile al numero di macchine meno una, diviso il numero di macchine. Nel caso SPT si tratta di una maggiorazione stretta che vale il doppio del numero di macchine, meno uno.

# Problemi di selezione di sottoinsiemi

Sono dati un insieme finito e una famiglia di suoi sottoinsiemi, a ciascuno dei quali è assegnato un costo. Si desidera determinare una sottofamiglia di sottoinsiemi rispettando dei vincoli, minimizzando il costo complessivo.
La famiglia è rappresentata da una matrice che ha per righe gli elementi e per colonne i sottoinsiemi. I valori della matrice sono pari a 1 per indicare l'appartenenza di un elemento ad un sottoinsieme, e zero altrove.

Si rappresenta l'appartenenza del sottoinsieme alla sottofamiglia dei sottoinsiemi tramite variabili decisionali binarie. Il costo della sottofamiglia, che è la funzione obiettivo, è la sommatoria dei costi dei sottoinsiemi scelti.

Esistono tre problemi principali di questo tipo: copertura, partizione e riempimento.

Il *problema di copertura* consiste nell'effettuare una scelta di sottoinsiemi tale da includere tutti gli elementi dell'insieme almeno una volta. Matematicamente, usando la matrice precedentemente definita, il vincolo di copertura impone che esista almeno un sottoinsieme che contenga ogni elemento, ovvero per ogni riga la sommatoria dei valori deve essere maggiore o uguale a uno. Il problema è formulabile come PLI. L'ottimalità della soluzione contenente un certo numero di sottoinsiemi si ottiene verificando che nessuna delle soluzioni possibili usando un sottoinsieme in meno sia ammissibile.

Il *problema di partizione* consiste nel scegliere la sottofamiglia affinché ogni elemento dell'insieme appaia in uno e un solo sottoinsieme. Questo significa che nella matrice ogni riga debba contenere esattamente un elemento di valore unitario e che tutti gli altri siano nulli. La sommatoria di ogni riga deve dunque essere esattamente uguale a uno.

Il *problema di riempimento* consiste nel determinare la famiglia di sottoinsiemi che massimizzi il numero di elementi scelti dell'insieme, evitando ripetizioni. Alcuni elementi potrebbero non essere presenti in nessuno dei sottoinsiemi. La funzione obiettivo è il numero di elementi scelti, e va massimizzata. Il *vincolo di riempimento* impone che per ogni riga della matrice debba esserci al più un uno, dunque la sommatoria di ciascuna riga deve essere minore o uguale a uno.

Per i problemi di copertura esiste una soluzuone *greedy*. La famiglia dei sottoinsiemi è inizializzata vuota. Ad ogni iterazione si seleziona in modo euristico un sottoinsieme non ancora considerato. Se esso copre almeno un elemento ancora scoperto, allora lo si aggiunge alla sottofamiglia. L'algoritmo termina dopo aver esaminato tutti i sottoinsiemi o dopo aver coperto tutti gli elementi. Le possibili euristiche possono essere basate sul costo non decrescente dei sottoinsiemi, sul costo medio non decrescente di copertura di un elemento mediante un sottoinsieme, oppure sui costi unitari attualizzati non decrescenti. Questi ultimi corrispondono al costo medio di copertura di un elemento non incluso, da parte del sottoinsieme considerato. I primi due criteri effettuano un solo ordinamento iniziale, che può essere scorso come una lista ordinata ad ogni iterazione in tempo $O(n \log n)$. Il terzo criterio deve ricostruire l'ordinamento ogni volta che un sottoinsieme viene inserito nella sottofamiglia. Questo criterio è solitamente il migliore perché è in grado di considerare la soluzione corrente come informazione.

La scelta di produzione mutuamente esclusiva su diverse linee o stabilimenti, con vincoli differenti a seconda del caso, non richiede la risoluzione di due problemi separati. È sufficiente utilizzare una variabile binaria che rappresenti la mutua esclusione tra i vincoli.
